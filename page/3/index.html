<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","Pisces | Gemini":240,"display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="JiangYh&#39;s Blog">
<meta property="og:url" content="http://example.com/page/3/index.html">
<meta property="og:site_name" content="JiangYh&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="JiangYH">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/3/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/3/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>JiangYh's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">JiangYh's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">JiangYH</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">31</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/yourname" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yourname" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%96%B9%E5%90%91/LLMs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="JiangYH">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%96%B9%E5%90%91/LLMs/" class="post-title-link" itemprop="url">大模型相关技术</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-12-13 23:03:48 / Modified: 23:09:11" itemprop="dateCreated datePublished" datetime="2023-12-13T23:03:48+08:00">2023-12-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li><a href="#%E5%A4%A7%E8%A7%84%E6%A8%A1%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E6%8C%91%E6%88%98">大规模训练技术挑战</a><ul>
<li><a href="#%E6%98%BE%E5%AD%98%E6%8C%91%E6%88%98">显存挑战</a></li>
<li><a href="#%E9%80%9A%E4%BF%A1%E6%8C%91%E6%88%98">通信挑战</a></li>
<li><a href="#%E8%AE%A1%E7%AE%97%E6%8C%91%E6%88%98">计算挑战</a></li>
<li><a href="#references">References</a></li>
</ul>
</li>
<li><a href="#%E5%BE%AE%E8%B0%83%E7%BB%8F%E9%AA%8C%E4%B8%8E%E6%8A%80%E6%9C%AF%E8%AF%B4%E6%98%8E">微调经验与技术说明</a><ul>
<li><a href="#%E7%BB%8F%E9%AA%8C%E6%96%B9%E6%B3%95">经验方法</a></li>
<li><a href="#%E6%8A%80%E6%9C%AF%E5%88%86%E7%B1%BB%E8%AF%B4%E6%98%8E">技术分类说明</a></li>
<li><a href="#references-1">References</a></li>
</ul>
</li>
<li><a href="#lora%E6%8A%80%E6%9C%AF%E4%BB%8B%E7%BB%8D">LoRA技术介绍</a><ul>
<li><a href="#%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF"><strong>研究背景</strong></a></li>
<li><a href="#%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82">技术细节</a></li>
<li><a href="#references-2">References</a></li>
</ul>
</li>
<li><a href="#chain-of-thought">Chain of Thought</a><ul>
<li><a href="#%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF-1"><strong>研究背景</strong></a></li>
<li><a href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C">相关工作</a></li>
<li><a href="#cot%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7">CoT的局限性</a></li>
<li><a href="#references-3">References</a></li>
</ul>
</li>
<li><a href="#llm-tricks">LLM-Tricks</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3">数据相关</a><ul>
<li><a href="#self-instruct">self-instruct</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#in-context-learning">In-Context learning</a><ul>
<li><a href="#references-4">References</a></li>
</ul>
</li>
<li><a href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9B%B8%E5%85%B3%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9">大模型相关基础内容</a><ul>
<li><a href="#%E7%AE%97%E5%8A%9B%E5%8C%BA%E5%88%86">算力区分</a><ul>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E9%9D%A2%E7%BB%8F">面经</a></li>
</ul>
<hr>
<h1 id="大规模训练技术挑战"><a href="#大规模训练技术挑战" class="headerlink" title="大规模训练技术挑战"></a>大规模训练技术挑战</h1><ul>
<li>分布式训练：</li>
</ul>
<p>数据并行：每张卡部分数据</p>
<p>模型并行：将模型每一层是现成可以并行到多卡实现的形式，进而将单层的计算可以切分到多卡上</p>
<p>流水并行：对模型块切分，将不同的层放到不同的卡上</p>
<p>挑战：</p>
<ul>
<li>显存墙</li>
</ul>
<p>模型比较大，单卡无法承载模型，需要用模型并行以及流水并行才能训练模型，但是<strong>会降低CPU的运算强度</strong></p>
<ul>
<li>计算墙</li>
</ul>
<p>大数据+大模型-&gt;巨大计算量。但由于显存墙的缘故，单卡运算强度低，多卡加速比较差-&gt;再多资源也可能无法训练完</p>
<h2 id="显存挑战"><a href="#显存挑战" class="headerlink" title="显存挑战"></a>显存挑战</h2><p>模型训练对显存的占用可以分为两部分：一部分是模型 forward 时保存下来的临时变量，这部分显存会在反向传播时会逐渐释放掉，这部分一般被称为 Activations。另一部分则是参数、梯度等状态信息占用的显存，这部分一般被称为 Model States。</p>
<ol>
<li>前向计算的过程是最占用显存的，降低这部分的峰值就能够给不超过显存墙</li>
<li><strong>短板效应</strong>-Model states 和 Activations 都有可能造成显存墙问题。它们相互独立但又相互制约。任意一侧的增大都会导致留给另一侧的显存空间变小，所以单单对一侧做优化是不够的，必须同时优化 Model states 和 Activations。</li>
<li>Transformer的大矩阵乘法能够拆分做模型并行，可以降低Activations的占用。</li>
</ol>
<h2 id="通信挑战"><a href="#通信挑战" class="headerlink" title="通信挑战"></a>通信挑战</h2><p>需要将切分的训练信息做聚合，问题：</p>
<ul>
<li>更新频繁，但传输速率远比不上加速芯片的运算速率；</li>
<li>机器规模较大的时候，基于 Ring-AllReduce 的通信聚合方式所构造的 Ring 将越来越大(节点越多通信量越大、延迟越高)，延迟将不可接受。</li>
<li>需要通信的梯度较多，带宽扛不住；多种并行也增加了通信的压力</li>
<li>大部分采用同步的通信步调，导致短板效应明显，单卡波动以及通信延迟导致问题变得更加严重</li>
</ul>
<ol>
<li>直接增大宽带-无法解决</li>
</ol>
<p>受限于网络协议，宽带的利用率不够高；</p>
<h2 id="计算挑战"><a href="#计算挑战" class="headerlink" title="计算挑战"></a>计算挑战</h2><p>需要较大的算力，但各种技术也会降低计算资源的利用率，需要考虑怎样提高计算效率</p>
<ul>
<li>Operator-level</li>
</ul>
<p>算子级别优化，需要解决的问题：</p>
<p><code>小算子过多</code>，<code>Kernel实现不够高效</code>，<code>内存局部性差</code></p>
<ul>
<li>Graph-level</li>
</ul>
<p>计算图优化，加速大规模训练，需要解决：</p>
<p>如何搜索出计算效率更高的计算图，如何用计算编译技术解决小算子问题，如何进行通信和计算的overlap 等</p>
<ul>
<li>Task-level</li>
</ul>
<p>训练阶段系统设计-实现一个计算效率最高的系统设计</p>
<ol>
<li>优秀分布式训练架构-扩展性强、节点很多也能保持较高加速比</li>
<li>平衡显存优化和速度优化</li>
</ol>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/350707888">大规模训练系列之技术挑战</a></li>
</ol>
<hr>
<h1 id="微调经验与技术说明"><a href="#微调经验与技术说明" class="headerlink" title="微调经验与技术说明"></a>微调经验与技术说明</h1><h2 id="经验方法"><a href="#经验方法" class="headerlink" title="经验方法"></a>经验方法</h2><ol>
<li>Freeze方法</li>
</ol>
<p>Freeze方法，即参数冻结，对原始模型部分参数进行冻结操作，仅训练部分参数，以达到在单卡或不进行TP或PP操作，就可以对大模型进行训练。</p>
<ol start="2">
<li>P-Tuning方法</li>
</ol>
<p><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%96%B9%E5%90%91/LLMs/image.png" alt="Alt text"></p>
<p>一种针对于大模型的soft-prompt方法。</p>
<p><code>P-Tuning</code>，仅对大模型的Embedding加入新的参数。</p>
<p><code>P-Tuning-V2</code>，将大模型的Embedding和每一层前都加上新的参数。</p>
<ol start="3">
<li>lora方法</li>
</ol>
<p><img src="https://pic1.zhimg.com/80/v2-e9965e231005d772d5ea5a6d2351fe60_720w.webp" alt="LORA"></p>
<p>在大型语言模型上对指定参数增加额外的低秩矩阵,并在模型训练过程中，仅训练额外增加的参数。当“秩值”远小于原始参数维度时，新增的低秩矩阵参数量很小，达到仅训练很小的参数，就能获取较好的结果。</p>
<h2 id="技术分类说明"><a href="#技术分类说明" class="headerlink" title="技术分类说明"></a>技术分类说明</h2><p><img src="https://pic3.zhimg.com/80/v2-a77957296da331dd9fcbdc9cf6efedda_720w.webp" alt="ft技术概括"></p>
<ol>
<li><p>fine-tuning技术</p>
<ul>
<li><p>(无监督)预训练+finetune的方式，实现对不同任务的适应，这是比较common的方法；</p>
</li>
<li><p>另一种则是采用迁移学习，对网络实现冻结，仅更新全连接层，其他层权重不变；</p>
</li>
</ul>
</li>
</ol>
<p><img src="https://pic4.zhimg.com/v2-c489f2b00e65c0a039fadbc2eaa27a9b_r.jpg" alt="当前FT方式"></p>
<ol start="2">
<li>parameter-efficient fine-tuning技术(PEFT)</li>
</ol>
<p>旨在在尽可能减少所需的参数和计算资源的情况下，实现对预训练语言模型的有效微调。</p>
<ul>
<li>蒸馏：学生模型(小模型)模仿教师模型(大模型)</li>
<li>适配器训练(adapter training)：适配器是添加到预训练模型中的小型神经网络，用于特定任务的微调。这些适配器只占原始模型大小的一小部分，这使得训练更快，内存需求更低。适配器可以针对多种任务进行训练，然后插入到预训练模型中以执行新任务。eg:lora</li>
<li>渐进收缩(progressive shrinking)：FT期间逐渐减小预训练模型，减少模型参数量的同时保证模型的性能。</li>
</ul>
<ol start="3">
<li>prompt-tuning技术</li>
</ol>
<p>重点是调整输入提示（input prompt）而非修改模型参数，即不会对原有的参数做任何修改，只有输入提示被修改以适应下游的任务。</p>
<p>相比于FT优势：</p>
<ol>
<li>计算成本和资源、时间等更少</li>
<li>更加灵活</li>
</ol>
<p>相关技术：</p>
<ol>
<li><p>Prefix tuning（前缀调整）</p>
<p>对特定任务学习连续提示，通过优化这个提示表示特征，模型能够在不修改底层模型的前提下实现不同的任务。</p>
</li>
<li><p>P-Tuning</p>
<blockquote>
<p>不同在于这个对位置没有特定的要求</p>
</blockquote>
<p>P-Tuning涉及训练可学习的称为“提示记号”的参数，这些参数与输入序列连接。这些提示记号是特定于任务的，在精调过程中进行优化，使得模型可以在保持原始模型参数不变的情况下在新任务上表现良好。</p>
</li>
</ol>
<h2 id="References-1"><a href="#References-1" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/620885226">大模型LLM-微调经验分享&amp;总结</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/620618701">预训练大语言模型的三种微调技术总结：fine-tuning、parameter-efficient fine-tuning和prompt-tuning的介绍和对比</a></li>
</ol>
<hr>
<h1 id="LoRA技术介绍"><a href="#LoRA技术介绍" class="headerlink" title="LoRA技术介绍"></a>LoRA技术介绍</h1><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><!-- 研究点与创新点 -->

<p>在LoRA方法提出之前，也有很多方法尝试解决大模型微调困境的方法。其中有两个主要的方向：</p>
<p>(1) 添加adapter层；</p>
<p>(2) 由某种形式的输入层激活。</p>
<p>但是这两种方法都有局限性：</p>
<ol>
<li><p>Adapter层会引入推理时延<br><img src="https://pic4.zhimg.com/80/v2-472ef7673e47745a75d4cd994e39e883_720w.webp" alt="adapter"></p>
</li>
<li><p>prefix-tuning难以优化<br><img src="https://pic2.zhimg.com/80/v2-2fd28b31d2a3261fa17c50cdaee19a05_720w.webp" alt="prefix-tuning"></p>
</li>
</ol>
<p>prefix-tuning方法是受语言模型in-context learning能力的启发，只要有合适的上下文则语言模型可以很好的解决自然语言任务。但是，针对特定的任务找到离散token的前缀需要花费很长时间，prefix-tuning提出使用连续的virtual token embedding来替换离散token。</p>
<h2 id="技术细节"><a href="#技术细节" class="headerlink" title="技术细节"></a>技术细节</h2><p><img src="https://img2023.cnblogs.com/blog/532548/202304/532548-20230417092405594-869232115.png" alt="LoRA实现"></p>
<p>总体概括：LoRA的基本原理是冻结预训练好的模型权重参数，在冻结原模型参数的情况下，通过往模型中加入额外的网络层，并只训练这些新增的网络层参数。由于这些新增参数数量较少，这样不仅 finetune 的成本显著下降，还能获得和全模型微调类似的效果。</p>
<p>其中，增加的是低秩分解矩阵，参数量小，也不会增加推理延迟。在实现过程中，会将该矩阵注入到transformer的每一层。</p>
<p>实现说明：<br>通常，神经网络中会包含许多进行矩阵乘法的稠密层，这些层通常是满秩的。相关研究表示其实预训练语言模型具有低的“内在维度”，受该工作的启发，在模型适配下游任务的过程中，权重更新也应该具有低的“内在秩”。</p>
<p>优点：显存和存储空间的减少。可以在部署时以更低的成本切换任务，仅需要交换LoRA权重即可。</p>
<h2 id="References-2"><a href="#References-2" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/618073170">【自然语言处理】【大模型】极低资源微调大模型方法LoRA以及BLOOM-LORA实现代码</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/LittleHann/p/17318509.html#_label0">LoRA（Low-Rank Adaptation of Large Language Models）– 一种大模型prompt-tuning调优方法</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/620552131">LoRA：大语言模型参数高效性微调方法</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/650197598">深入浅出 LoRA - 知乎 (zhihu.com)</a></li>
</ol>
<hr>
<h1 id="Chain-of-Thought"><a href="#Chain-of-Thought" class="headerlink" title="Chain of Thought"></a>Chain of Thought</h1><p>思维链主要用于提升模型的逻辑推理能力，使得AI能够有类似于人一样的推理能力。</p>
<h2 id="研究背景-1"><a href="#研究背景-1" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><!-- 研究点与创新点 -->
<p>最早的相关工作是few-shot,one-shot,zero-shot等在推理时能够提供不同量的样本，使得模型的推理能力能够有进一步的提升。但这种方法依旧存在较大的问题，如果你的问题相对简单，不需要什么逻辑推理，可能靠大模型背答案就能做得不错，但是对于一些需要推理的问题，都不用太难，就一些简单的算术应用题，大模型就大概率不太 work。</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><ul>
<li>CoT<blockquote>
<p><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2005.14165">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></p>
</blockquote>
</li>
</ul>
<p><img src="https://pic1.zhimg.com/80/v2-fb5a82b4689c8cfa03379636a07f7798_1440w.webp" alt="CoT"><br>简单来说就是将原本的问题，经过多个中间步骤最终获取答案，实现更好的推理。</p>
<p>具体实现效果：常识推理能力赶超人类；数学逻辑推理能力大幅度提升；LLM可解释性更强。</p>
<ul>
<li>Zero-shot-CoT</li>
</ul>
<p><img src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/AIR6eRePgjMhNjfkrLWwVVG5VcrEBicc7ZhIQ7BQZySSOQQPXsDSzI5SFBqtzJy1qWg8oKbiamVe4gYnWUGb0KEw/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1" alt="zsCoT"></p>
<p>零样本思维链通过引入与样本无关指示，来实现自我增强</p>
<ul>
<li>多数投票提高CoT性能——自洽性（Self-consistency）</li>
</ul>
<p>其实核心就是对生成的多个结果选择取多数的答案，这一个可以直接通过控制temprature和Top-K来实现，很显然这会使得时间会变长。</p>
<ul>
<li>LtM(Least to Most prompting)</li>
</ul>
<p><img src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/AIR6eRePgjMhNjfkrLWwVVG5VcrEBicc7az1RYAvicTudsqYfa5lmNg5nXZTIYqmCibrcLe101pazQVMMerEXtc4Q/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1" alt="LtM"></p>
<p>将问题按步骤拆分成多个子问题，解决完多个子问题后回答最终问题。具体训练就是分为多个CoT阶段实现。</p>
<ul>
<li>Flan-PaLM&#x2F;T5：CoT + Finetuning</li>
</ul>
<p>Flan-T5：在超大规模的任务上对模型进行<strong>微调</strong>，使得单个模型在1800多个NLP任务上都能够有很好的表现。</p>
<p>微调方法就是在加入CoT数据。其核心是对多任务数据的统一。</p>
<p>实现流程：</p>
<ol>
<li>收集带有标签的数据，将每个任务定义为&lt;数据，任务类型&gt;</li>
<li>对数据的形式进行改写，比如改写成CoT的形式；并对是否需要CoT和few-shot，进行组合构造</li>
<li>训练过程：恒定的学习率以及 Adafactor 优化器；同时将多个训练样本打包成一个训练样本，通过特殊结束token进行分割。</li>
</ol>
<p>结论：</p>
<ol>
<li>微调有效果，模型越大越好，任务越多越好</li>
<li>混杂CoT很重要</li>
</ol>
<ul>
<li>提升小模型的推理能力：Fine-tune-CoT</li>
</ul>
<p>旨在利用大模型思维链推理能力指导小模型解决复杂问题。</p>
<p><img src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/AIR6eRePgjMhNjfkrLWwVVG5VcrEBicc7a9toDzxA7MFzQA2ZZCSBTKIwjoicz5lmnXg0us7P206gvOdehxyoZeQ/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1" alt="FuCoT"></p>
<p>简单的说就是用ChatGPT这类大模型生成CoT数据，然后再喂给小模型进行微调。同时该方法需要生成尽可能多的数据。</p>
<h2 id="CoT的局限性"><a href="#CoT的局限性" class="headerlink" title="CoT的局限性"></a>CoT的局限性</h2><ol>
<li>思维链只有在模型规模足够大的时候才适用，如何实现小模型的思维链应用是值得探索的方向</li>
<li>应用领域有限，当前的实验结果只是在部分领域有所评估。另外，思维链只是提高模型的推理能力，但不代表模型真正理解内在的逻辑。</li>
</ol>
<h2 id="References-3"><a href="#References-3" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/BMv_FVJ3j6q71LqA4fUYUA">【他山之石】大模型思维链（Chain-of-Thought）技术原理</a></li>
</ol>
<h1 id="LLM-Tricks"><a href="#LLM-Tricks" class="headerlink" title="LLM-Tricks"></a>LLM-Tricks</h1><h2 id="数据相关"><a href="#数据相关" class="headerlink" title="数据相关"></a>数据相关</h2><h3 id="self-instruct"><a href="#self-instruct" class="headerlink" title="self-instruct"></a>self-instruct</h3><blockquote>
<p>Self-Instruct: Aligning Language Model with Self Generated Instructions</p>
</blockquote>
<p>基于指令框架降低人工标注指令数据的成本。</p>
<p>相关工作-人工标注：</p>
<ul>
<li>人工设计相关指令任务</li>
<li>对当前指令任务进行标注(编写正确答案)</li>
</ul>
<p>当前工作self-instruct：</p>
<ul>
<li>人工设计175个表示不同任务的指令(完整输入输出)，将这部分数据作为种子池</li>
<li>使用模型生成新的指令：6个人工指令+2个生成指令-》生成新的指令</li>
<li>对该模型生成的指令判断是否分类任务：prompt模板会根据是否是分类任务有所不同。</li>
<li>使用模型生成实例：输入优先以及输出优先(分类)两种输出策略。</li>
<li>对上述模型生成的数据进行过滤和后处理：ROUGE-L&lt;0.7才加入-保证多样性，减少重复内容；排除一些无法处理的指令；过滤输入相同但输出不同的实例。</li>
<li>将经过过滤和后处理的数据添加到种子池中；<br>一直重复上述2到6步直到种子池有足够多的数据；</li>
</ul>
<p>对于分类任务，如果先生成文本，后生成标签，<strong>模型会偏向于生成比较单一的结果</strong>。所以对于分类任务，是先生成随机的标签，然后再生成该标签对应的文本。</p>
<p>指标ROUGE-L:最长连续公共子串占比对两个字符串的比值，再通过F1计算</p>
<p>结论：GPT3+self-instruct性能与text-davinci-001接近；self-instruct是有一定性能提升的；数据集不大-252条指令</p>
<blockquote>
<ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/614916562">Self-Instruct: Aligning Language Model with Self Generated Instructions</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/504279252">NLP评估指标之ROUGE</a></li>
</ol>
</blockquote>
<h1 id="In-Context-learning"><a href="#In-Context-learning" class="headerlink" title="In-Context learning"></a>In-Context learning</h1><h2 id="References-4"><a href="#References-4" class="headerlink" title="References"></a>References</h2><p>1.<a target="_blank" rel="noopener" href="https://www.cnblogs.com/LittleHann/p/17318509.html#_label0"></a></p>
<h1 id="大模型相关基础内容"><a href="#大模型相关基础内容" class="headerlink" title="大模型相关基础内容"></a>大模型相关基础内容</h1><h2 id="算力区分"><a href="#算力区分" class="headerlink" title="算力区分"></a>算力区分</h2><p>FLOPS（Floating-Point Operations Per Second） - 这是衡量计算机或其他设备执行浮点运算速度的基本单位，表示每秒钟可以执行多少次浮点运算（加、减、乘和除等运算）。FLOPS 以前通常用于衡量大规模科学计算和数值模拟等需要双精度浮点数计算的应用程序，现在也被用于描述AI高精度训练算力。</p>
<p>FP64：双精度浮点数，占用64位存储空间，通常用于大规模科学计算、工程计算等需要高精度计算的算法。</p>
<p>FP32：单精度浮点数，占用32位存储空间。与双精度浮点数相比，存储空间较小但精度较低，部分科学计算和工程计算也可以使用FP32，但通常也用于神经网络的前向推理和反向传播计算。</p>
<p>FP16：半精度浮点数，占用16位存储空间。存储空间更小但精度进一步降低，通常用于模型训练过程中参数和梯度的计算。</p>
<p>BF16: 用于<strong>半精度矩阵乘法计算</strong>（GEMM）的浮点数格式，占用16位存储空间。相对于FP16，在保持存储空间相同的情况下能够提高运算精度和效率。</p>
<p>TF32：TensorFLoat-32，是NVIDIA定义的使用TensorCore的中间计算格式。</p>
<p>INT8：8位整数，用于量化神经网络的计算，由于存储和计算都相对于浮点数更加高效，在低功耗、嵌入式系统和边缘设备等领域有着广泛的应用。用TOPS（Tera Operations Per Second，每秒处理的万亿级别的操作数）作为计算性能的单位。</p>
<p>INT4：4位整数，只能表示-8到7的16个整数。因为新的量化技术出现，追求更低的存储空间，减少计算量和更高的算力密度，而产生的新格式。</p>
<p>量化：本质上只是对数值范围的重新调整，可以「粗略」理解为是一种线性映射。</p>
<p>CUDA内核对INT8处理不是十分高效，INT8计算难以使得GPU核心饱和，且由于需要额外的量化开销，因此会减慢整体的推理速度。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/604338403">【自然语言处理】【大模型】用于大型Transformer的8-bit矩阵乘法介绍</a>-<a target="_blank" rel="noopener" href="https://huggingface.co/blog/zh/hf-bitsandbytes-integration">原博客</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/TimDettmers/bitsandbytes/issues/6#issuecomment-1211345635">int8速度慢原理分析</a></li>
</ol>
<h1 id="面经"><a href="#面经" class="headerlink" title="面经"></a>面经</h1><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/643829565">https://zhuanlan.zhihu.com/p/643829565</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/643836163">https://zhuanlan.zhihu.com/p/643836163</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/643560888">https://zhuanlan.zhihu.com/p/643560888</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="JiangYH">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" class="post-title-link" itemprop="url">注意力机制笔记</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-12-13 22:56:46 / Modified: 22:58:36" itemprop="dateCreated datePublished" datetime="2023-12-13T22:56:46+08:00">2023-12-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li><a href="#transformer">Transformer</a><ul>
<li><a href="#references">References</a></li>
</ul>
</li>
<li><a href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C">位置编码相关工作</a><ul>
<li><a href="#%E6%80%BB%E8%BF%B0">总述</a><ul>
<li><a href="#%E7%BB%9D%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81">绝对位置编码</a></li>
<li><a href="#%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81">相对位置编码</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
</li>
<li><a href="#rope">RoPE</a></li>
</ul>
</li>
<li><a href="#%E4%B8%8D%E5%90%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E5%99%A8%E8%AE%BE%E8%AE%A1">不同激活函数与优化器设计</a><ul>
<li><a href="#%E4%BC%98%E5%8C%96%E5%99%A8">优化器</a></li>
<li><a href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">激活函数</a><ul>
<li><a href="#swiglu">SwiGLU</a></li>
<li><a href="#%E5%8F%82%E8%80%83-1">参考</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E5%BE%AE%E8%B0%83">模型结构微调</a><ul>
<li><a href="#norm">Norm</a><ul>
<li><a href="#pre-norm">Pre-Norm</a></li>
<li><a href="#norm%E7%B1%BB%E5%9E%8B">Norm类型</a></li>
<li><a href="#%E5%8F%82%E8%80%83-2">参考</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E4%B8%8D%E5%90%8Cattention%E5%AE%9E%E7%8E%B0">不同attention实现</a><ul>
<li><a href="#flash-attention">Flash-Attention</a></li>
<li><a href="#pageattention">PageAttention</a></li>
<li><a href="#multi-query-attention">Multi-Query Attention</a></li>
<li><a href="#group-query-attention">Group-Query Attention</a></li>
</ul>
</li>
<li><a href="#bert%E4%B8%8Egpt%E7%9A%84%E4%B8%8D%E5%90%8C">Bert与GPT的不同</a><ul>
<li><a href="#%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B">自回归模型</a><ul>
<li><a href="#gpt%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B">GPT系列模型</a></li>
</ul>
</li>
<li><a href="#%E8%87%AA%E7%BC%96%E7%A0%81%E6%A8%A1%E5%9E%8B">自编码模型</a></li>
<li><a href="#encoder-decoder">Encoder-Decoder</a></li>
<li><a href="#%E5%8F%82%E8%80%83-3">参考</a></li>
</ul>
</li>
</ul>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>结构篇：</p>
<p><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/640.png" alt="transformer结构"></p>
<p>encoder-decoder:6 block</p>
<p>相关问题</p>
<ol>
<li><p>self-attention实现</p>
<p> $Softmax\frac{(Q*k)}{\sqrt{d_k}}V$，其中$d_k$是Q,K的列数，防止内积过大；可以使得输入的数据的分布变得更好，防止梯度消失，让模型能够更容易训练。</p>
<p> 只要能够建模相关性，别的建模方式也能够代替当前的自注意力计算；同样的能够缓解梯度消失问题也不用除掉列数值。</p>
<p> 对位置信息不敏感，需要增加pos-emb；embedding 的直接相加,类似于信号的叠加，只要保证频率不同叠加的信号就能够再后续发挥作用。</p>
<p> QKV的不同主要是为了增强容量和表达能力。多头也是为了增加参数量进而增强模型的表达能力，</p>
</li>
<li><p>整体的维度变化</p>
<p> input:(bs,max_len)<br> embedding:(bs,max_len,hidden_size)<br> MHA:(bs,max_len,hidden_size)</p>
<pre><code> Q(K,V): (bs,max_len,hidden_size)
 多头机制：
 input:(bs*num_heads,max_len,hidden_size//num_heads)
 output:(bs*num_heads,max_len,hidden_size//num_heads)
 concat&amp;Linear:(bs,max_len,hidden_size)
</code></pre>
<p> add&amp;Post-Norm:(bs,max_len,hidden_size)<br> FF:(bs,max_len,hidden_size)</p>
<pre><code> 先升维再降维
 FF1:(bs,max_len,hidden_size*4)
 FF2:(bs,max_len,hidden_size)
</code></pre>
</li>
<li><p>计算复杂度对比</p>
</li>
</ol>
<p><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/v2-7792863eb59d96636cbddbf85788c1c4_1440w.webp" alt="img"></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzAxMTU5Njg4NQ==&mid=2247490647&idx=1&sn=a711ed1c556cdae1821bab251ee3893f&chksm=9bbff96dacc8707bdb133acfa55d599ca8dcea47027aa1fc2344d06d84584dbd4981c939fee0&scene=21#wechat_redirect">【关于Transformer】 那些的你不知道的事（上）</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/neumy/p/15932615.html">从Attention 到 MultiHeadAttention</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/338817680">Transformer模型详解（图解最完整版）</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/hyzhyzhyz12345/article/details/104119375">说说transformer当中的维度变化</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/643560888">大模型面试八股</a></li>
</ol>
<h1 id="位置编码相关工作"><a href="#位置编码相关工作" class="headerlink" title="位置编码相关工作"></a>位置编码相关工作</h1><h2 id="总述"><a href="#总述" class="headerlink" title="总述"></a>总述</h2><p>基于自注意力机制本身的计算原理，其对位置信息不敏感，具体从公式角度来看：</p>
<p><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image.png" alt="Attention-cal"></p>
<p>调换序列中两个元素的位置不会影响到当前的注意力得分计算。</p>
<h3 id="绝对位置编码"><a href="#绝对位置编码" class="headerlink" title="绝对位置编码"></a>绝对位置编码</h3><ul>
<li>训练式</li>
</ul>
<p>直接将位置编码当作可训练参数。一般的认为，该方法的缺点在于没有外推性，超过预设窗口大小的内容就无法处理了。(当前有一些可以通过如层次分解的方法将位置编码外推足够长的范围)</p>
<ul>
<li>三角式-transformer：</li>
</ul>
<p><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20230718230559320.png" alt="image-20230718230559320"></p>
<p><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20230718230640108.png" alt="image-20230718230640108"></p>
<p>通过内积的方法将相对位置信息融入到特征中。但具体实现中，由于参数矩阵也需要参与计算$p^{T}<em>{t}W^{T}</em>{Q}W_{K}p_s \not &#x3D;{p^{T}_{t}p_s}.$经相关研究，可知由于参数矩阵使得余弦波不再是理想情况，无法真正感知元素的相对未知信息。</p>
<ul>
<li>递归式</li>
</ul>
<p>本质思想是RNN这类递归模型，学习位置编码，再接入transformer。但最大的问题是牺牲了一定的并行性，会带来速度瓶颈。</p>
<h3 id="相对位置编码"><a href="#相对位置编码" class="headerlink" title="相对位置编码"></a>相对位置编码</h3><p>相对位置编码在计算自注意力矩阵时，根据矩阵元素的下标，直接考虑每个元素对应的两个token间的相对位置关系。此外，相比于绝对位置编码仅仅在输入层考虑顺序特征，相对位置编码则通过修改自注意力计算的过程，植入到Transformer架构的每一层。</p>
<p>不同模型的相对位置编码：</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/631363482">Transformer位置编码（基础）</a></li>
<li><a target="_blank" rel="noopener" href="https://kexue.fm/archives/8130">让研究人员绞尽脑汁的Transformer位置编码</a></li>
<li></li>
</ol>
<h2 id="RoPE"><a href="#RoPE" class="headerlink" title="RoPE"></a><a target="_blank" rel="noopener" href="https://gitee.com/Jonny-Jaia/ready-blog/blob/master/%E6%9D%82/4-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E4%B9%A0/LongContextWindow.md#rope">RoPE</a></h2><h1 id="不同激活函数与优化器设计"><a href="#不同激活函数与优化器设计" class="headerlink" title="不同激活函数与优化器设计"></a>不同激活函数与优化器设计</h1><h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p>AdamW</p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="SwiGLU"><a href="#SwiGLU" class="headerlink" title="SwiGLU"></a>SwiGLU</h3><ol>
<li>siwish-线性函数与ReLU之间的平滑</li>
</ol>
<p>$f(x)&#x3D;x \times sigmoid(\beta x)$</p>
<ol start="2">
<li>GELU-高斯误差线性单元，RELU的变种</li>
</ol>
<p>$f(x)&#x3D;x \times \phi(x),\phi(x)是正态分布的累积函数$，和Swish形式性质相似，表现相当</p>
<ol start="3">
<li>GLU-门控<br>$\text{GLU}(a, b) &#x3D; a \otimes \sigma(b)$<br><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/v2-1520514ff5f2c6ea2a0def6ace10bb42_720w.png"></li>
</ol>
<p>具体就是首先通过中间向量g(x)&#x3D;xW进行门控操作，使用Sigmoid函数σ将其映射到0到1之间的范围，表示每个元素被保留的概率。然后，将输入向量x与门控后的向量进行逐元素相乘（即 ⊗ 操作），得到最终的输出向量。</p>
<ol start="4">
<li>GEGL-GLU变体</li>
</ol>
<p>就是将GLU中的sigmoid激活函数替换成GELU激活函数<br>5. SwiGLU<br><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/v2-a7a7b4ad7ce1018183e1b8eb654f3f91_1440w.webp" alt="swishglu"></p>
<p>就是将GLU中的sigmoid激活函数替换成Swish激活函数</p>
<h3 id="参考-1"><a href="#参考-1" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/638735903">大语言模型综述&lt;演进，技术路线，区别，微调，实践，潜在问题与讨论&gt;</a></li>
</ol>
<h1 id="模型结构微调"><a href="#模型结构微调" class="headerlink" title="模型结构微调"></a>模型结构微调</h1><h2 id="Norm"><a href="#Norm" class="headerlink" title="Norm"></a>Norm</h2><h3 id="Pre-Norm"><a href="#Pre-Norm" class="headerlink" title="Pre-Norm"></a>Pre-Norm</h3><ul>
<li>LayerNorm会影响训练的稳定性</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://readpaper.com/pdf-annotate/note?pdfId=4500336028413485057&noteId=1867628851738139648">Megatron-LM</a> 用实验证明layernorm后置效果要更加稳定</p>
</blockquote>
<p><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20230712101727279.png" alt="image-20230712101727279"></p>
<p>Post-LN（原始的BERT）</p>
<p>Pre-LN：On layer normalization in the transformer architecture</p>
<p>Sandwich-LN: Cogview: Mastering text-to-image generation via transformers</p>
<p>通常认为稳定性上: Sandwich-LN &gt; Pre-LN &gt; Post-LN</p>
<h3 id="Norm类型"><a href="#Norm类型" class="headerlink" title="Norm类型"></a>Norm类型</h3><ul>
<li><p>LayerNorm<br>传统transformer-Post-LN、随着层数加深梯度范数会增大导致训练不稳定。<br>Pre-LN:使用pre LN的深层transformer训练更稳定，可以缓解训练不稳定问题。但缺点是pre LN可能会轻微影响transformer模型的性能 大语言模型的一个挑战就是如何提升训练的稳定性。<br><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/v2-dcd81bd8ea2fcba1e777f700ac1e5146_1440w.webp" alt="LayerNorm"></p>
<p>Norm中采用的性质：</p>
<ol>
<li>平移不变性：均值</li>
<li>缩放不变性：方差</li>
</ol>
</li>
<li><p>RMSNorm<br>只保留缩放，简化计算的同时，效果基本相当甚至还略有提升。<br><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/v2-f1eb53988e49ac7358d2af489ec4b9bd_1440w.webp" alt="RMSNorm"></p>
</li>
<li><p>DeepNorm</p>
<p><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/98a70456797347a68085cce705d71ed3.png" alt="deepnorm"></p>
<p>用以缓解爆炸式模型更新的问题，更可以再此基础上实现千层堆积。<br>$x &#x3D; LayerNorm(x \times \alpha + f(x))$</p>
<ol>
<li>DeepNorm在进行Layer Norm之前会以 α参数扩大残差连接</li>
<li>在Xavier参数初始化过程中以 β减小部分参数的初始化范围</li>
</ol>
</li>
</ul>
<h3 id="参考-2"><a href="#参考-2" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/638735903">大语言模型综述&lt;演进，技术路线，区别，微调，实践，潜在问题与讨论&gt;</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/479860623">DEEPNORM：千层transformer…</a></li>
</ol>
<h1 id="不同attention实现"><a href="#不同attention实现" class="headerlink" title="不同attention实现"></a>不同attention实现</h1><h2 id="Flash-Attention"><a href="#Flash-Attention" class="headerlink" title="Flash-Attention"></a><a target="_blank" rel="noopener" href="https://gitee.com/Jonny-Jaia/ready-blog/blob/master/%E6%9D%82/4-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E4%B9%A0/FlashAttention.md#flashattention">Flash-Attention</a></h2><h2 id="PageAttention"><a href="#PageAttention" class="headerlink" title="PageAttention"></a><a target="_blank" rel="noopener" href="https://gitee.com/Jonny-Jaia/ready-blog/blob/master/%E6%9D%82/4-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E4%B9%A0/LLM%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F.md#vllmpagedattention">PageAttention</a></h2><h2 id="Multi-Query-Attention"><a href="#Multi-Query-Attention" class="headerlink" title="Multi-Query Attention"></a><a href>Multi-Query Attention</a></h2><h2 id="Group-Query-Attention"><a href="#Group-Query-Attention" class="headerlink" title="Group-Query Attention"></a><a href>Group-Query Attention</a></h2><h1 id="Bert与GPT的不同"><a href="#Bert与GPT的不同" class="headerlink" title="Bert与GPT的不同"></a>Bert与GPT的不同</h1><h2 id="自回归模型"><a href="#自回归模型" class="headerlink" title="自回归模型"></a>自回归模型</h2><p>AR，代表GPT，从左向右学习。</p>
<p>AR模型通常用于生成式任务，在长文本的生成能力很强，比如自然语言生成（NLG）领域的任务：摘要、翻译或抽象问答。</p>
<p>具体来说，就是利用上文词预测下一个词的发生概率。</p>
<p>优点：AR模型擅长生成式NLP任务。AR模型使用注意力机制，预测下一个token，因此自然适用于文本生成。此外，AR模型可以简单地将训练目标设置为预测语料库中的下一个token，因此生成数据相对容易。</p>
<p>缺点：AR模型只能用于前向或者后向建模，不能同时使用双向的上下文信息，不能完全捕捉token的内在联系。</p>
<h3 id="GPT系列模型"><a href="#GPT系列模型" class="headerlink" title="GPT系列模型"></a>GPT系列模型</h3><ul>
<li><p>GPT1<br>通过无监督预训练+有监督微调实现模型性能的提升。另外，将预训练目标作为辅助目标加入下游任务loss中，将会提高有监督模型的泛化性能，并加速收敛。</p>
</li>
<li><p>GPT2-15B</p>
</li>
</ul>
<blockquote>
<p> “所有的有监督学习都是无监督语言模型的一个子集”</p>
</blockquote>
<p>  增大了模型大小与参数规模，提出了zero-shot，并且提出了以一个通用预训练模型为基础，使得下游任务无需手动生成或标记训练数据集，更不需要修改预训练模型的参数或结构。</p>
<p>  GPT2通过实验验证了海量数据与大量参数训练得到的语言模型可以迁移到下游其他任务中，无需额外训练和微调。</p>
<ul>
<li><p>GPT3-175B<br>引入了In-Context Learning的概念，GPT3参数量增大的同时，期望不通过微调直接能够通过上下文指示也能够有较好的性能。</p>
<p>In-Context learning是元学习（Meta-learning）的一种，元学习的核心思想在于通过<strong>少量的数据寻找一个合适的初始化范围</strong>，使得模型能够在有限的数据集上快速拟合，并获得不错的效果。</p>
</li>
<li><p>InstructGPT</p>
<blockquote>
<p>提出动机：让模型的输出达到3H(helpful,honest,harmless)</p>
</blockquote>
<ol>
<li>RLHF</li>
</ol>
<p>  人类喜欢的内容大致符合以上的3H标准，并且也能够保证生成内容流畅性与语法正确性；</p>
<p>  通过RL指导模型训练，以人类反馈作为奖励，实现将人类经验内容的注入。</p>
<ol start="2">
<li>实验步骤</li>
</ol>
<p>  有监督微调-基于人工标注的对比数据训练奖励模型-基于RM利用PPO微调SFT模型；</p>
<p>  三部分数据集：<br>  SFT数据：简单任务、few-shot任务、用户相关的任务；<br>  RM数据：让模型先生成一批候选文本，然后针对这部分数据进行排序；<br>  PPO数据：无标注数据，来自GPT3的API用户调用任务数据；</p>
<p>  训练设置：<br>  SFT：与GPT3一致，适当过拟合有助于后续的训练；<br>  RM：输入prompt和response，输出奖励值；训练过程中将同一个prompt的k个输出成对取出共有$C_{K}^{2}$个结果作为一个batch输入，loss就是最大化结果的差值；<br>  PPO：KL惩罚确保两个策略的输出差距不会很大；为了防止模型在通用NLP任务上性能大幅度下降，优化目标中增加了通用语言模型的目标；</p>
<ol start="3">
<li>优缺点</li>
</ol>
<p>  优点：结果更真实，无害性提高，coding能力提升；<br>  缺点：会降低在通用NLP任务上的效果；依然会给出奇怪的输出；对指示十分敏感；对简单概念过分解读。</p>
</li>
<li><p>GPT4-1.8T</p>
</li>
</ul>
<ol>
<li>模型架构</li>
</ol>
<p>采用的是MoE的架构</p>
<ol start="2">
<li>数据组成</li>
</ol>
<h2 id="自编码模型"><a href="#自编码模型" class="headerlink" title="自编码模型"></a>自编码模型</h2><p>AE，代表BERT，主要是对掩码部分能够实现重建，常用于内容理解任务，比如自然语言理解（NLU）中的分类任务：情感分析、提取式问答。</p>
<p>优点：在上下文依赖中，BERT的表示可以涵盖前后向两边的上下文。BERT使用双向transformer，在语言理解相关的任务中表现很好。</p>
<p>缺点：</p>
<ul>
<li>输入噪声：<br>BERT在预训练过程中使用【mask】符号对输入进行处理，这些符号在下游的finetune任务中永远不会出现，这会导致预训练-微调差异。而AR模型不会依赖于任何被mask的输入，因此不会遇到这类问题。</li>
<li>BERT在对联合条件概率进行因式分解时，基于一个独立假设：在给定了unmasked tokens时，所有待预测（masked）的tokens是相互独立的。</li>
</ul>
<h2 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h2><p>同时包含编码器和解码器两部分，常用的有T5、BART等模型</p>
<h2 id="参考-3"><a href="#参考-3" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/626494749">[万字长文]ChatGPT系列论文精读——大模型经典论文GPT1、GPT2、GPT3</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/625714067">一文读懂GPT家族和BERT的底层区别——自回归和自编码语言模型详解</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/642927542">大规模语言模型（LLMs）预训练十六: GPT-4大揭密</a></li>
<li><a target="_blank" rel="noopener" href="https://www.semianalysis.com/p/gpt-4-architecture-infrastructure">GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE (semianalysis.com)</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/590311003">ChatGPT&#x2F;InstructGPT详解</a></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="JiangYH">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/" class="post-title-link" itemprop="url">条件随机场</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-12-13 22:56:46 / Modified: 20:27:18" itemprop="dateCreated datePublished" datetime="2023-12-13T22:56:46+08:00">2023-12-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li><a href="#%E4%BB%8B%E7%BB%8D">介绍</a></li>
<li><a href="#%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95">维特比算法</a></li>
</ul>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>CRF:给定一组输入随机变量条件下另一组输出随机变量的<strong>条件概率分布模型</strong>-假设：输出变量构成马尔科夫随机场，其联合概率分布构成概率无向图模型。</p>
<p>CRF：判别模型</p>
<p>学习方法：极大似然估计或正则化的极大似然估计-最大化发生概率</p>
<p>常用解码方法：维特比算法</p>
<h1 id="维特比算法"><a href="#维特比算法" class="headerlink" title="维特比算法"></a>维特比算法</h1><p>CRF预测问题-非规范化概率最大的最优路径问题<br>即：$\max_{y} (w \cdot F(y,x)),w表示特征权值，F表示转移的特征$</p>
<ul>
<li>参数：输入-模型特征向量、权值向量、观测序列；输出：最优路径</li>
<li>初始化：起始标签-<code>dp[0][state] = start[state] * emission[state][x1]</code></li>
<li>递推-动态规划：计算每一个前一个状态prev_state到当前状态state的得分: <code>dp[t][state] = dp[t-1][prev_state] * transition[prev_state][state] * emission[state][xt]</code>从中选择最大的得分，同时记录使得得分最大的prev_state，以便后续进行路径回溯。</li>
<li>终止：找到在最后位置的最大得分及其对应的状态。</li>
<li>路径回溯:从最后位置的最优状态开始，使用之前存储的prev_state信息回溯找到整个最优路径。</li>
<li>输出最优路径。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="JiangYH">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/" class="post-title-link" itemprop="url">生成式模型学习</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-12-13 22:56:46 / Modified: 23:03:35" itemprop="dateCreated datePublished" datetime="2023-12-13T22:56:46+08:00">2023-12-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li><a href="#bart-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95">BART-预训练方法</a><ul>
<li><a href="#%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF"><strong>研究背景</strong></a></li>
<li><a href="#%E4%B8%BB%E8%A6%81%E5%B7%A5%E4%BD%9C"><strong>主要工作</strong></a></li>
<li><a href="#references">References</a></li>
</ul>
</li>
<li><a href="#t5">T5</a></li>
<li><a href="#bloom">BLOOM</a></li>
<li><a href="#gpt">GPT</a><ul>
<li><a href="#references-1">References</a></li>
</ul>
</li>
<li><a href="#cpt">CPT</a></li>
<li><a href="#%E8%A7%A3%E7%A0%81%E7%AE%97%E6%B3%95">解码算法</a><ul>
<li><a href="#refereces">Refereces</a></li>
</ul>
</li>
</ul>
<h1 id="BART-预训练方法"><a href="#BART-预训练方法" class="headerlink" title="BART-预训练方法"></a>BART-预训练方法</h1><blockquote>
<p>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</p>
</blockquote>
<h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><p>文本理解任务，大部分的工作范式都是采取语言模型预训练+下游任务finetune这一种方式。BART模型相当于采用了BERT+GPT的模型结构兼顾上下文语境信息的同时带有自回归特性。BART相当于是建立在seq2seq Transformer model的基础上，使其分别能够适用于文本生成和文本理解的任务，并在这些任务都实现了较好的性能。</p>
<h2 id="主要工作"><a href="#主要工作" class="headerlink" title="主要工作"></a><strong>主要工作</strong></h2><ol>
<li>模型结构BERT+GPT，兼顾理解与生成；</li>
<li>BART的预训练任务是恢复基于随机噪声破坏后的文本；</li>
<li>具体的noise有5种：<ul>
<li>token mask:单个掩码</li>
<li>token deletion:随机删去token</li>
<li>text infilling:随机将一段连续的token（称作span）替换成一个[MASK]，span的长度服从$$ \lambda&#x3D;3 $$的<strong>泊松分布</strong>。注意span长度为0就相当于插入一个[MASK]。</li>
<li>Sentence Permutation: 将一个document的句子打乱</li>
<li>Document Rotation: 从document序列中随机选择一个token，然后使得该token作为document的开头</li>
</ul>
</li>
<li>适用于不同的下游任务</li>
</ol>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/173858031">【论文精读】生成式预训练之BART</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.13461.pdf">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></li>
</ol>
<h1 id="T5"><a href="#T5" class="headerlink" title="T5"></a>T5</h1><h1 id="BLOOM"><a href="#BLOOM" class="headerlink" title="BLOOM"></a>BLOOM</h1><h1 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h1><h2 id="References-1"><a href="#References-1" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/350017443">预训练语言模型之GPT-1，GPT-2和GPT-3</a></li>
</ol>
<h1 id="CPT"><a href="#CPT" class="headerlink" title="CPT"></a>CPT</h1><h1 id="解码算法"><a href="#解码算法" class="headerlink" title="解码算法"></a>解码算法</h1><ol>
<li><p>greeedy-贪心</p>
<p> 直接logit最大</p>
</li>
<li><p>beam search<br> <img src="https://pic3.zhimg.com/80/v2-0163d5ba0ea18faf9fb4b63b93b6c906_720w.webp" alt="beam-search"><br> 始终从k条最优序列中选择K个最好的</p>
<p> beach search需要选择beam size，越小越接近greedy，beam size&#x3D;1就等价于greedy. 越大beam size 计算代价就越大，并且有些试验告诉我们太大的beam size可能会有更差的效果。一般来说beam size&#x3D;3是一个不错的trade off。</p>
</li>
<li><p>sampling-based</p>
<p> 每次算出最有可能的token集合，然后按照某种概率分布从中采样n个候选答案。和beam search区别是用softmax代替argmax，通常用于开放式或者需要创造性的生成任务上，比如写故事&#x2F;写诗之类。</p>
</li>
<li><p>softmax temperature</p>
<p> Softmax temperature是在softmax层上加了一个超参数，可以用来平衡diverse的。Softmax temperature是调节softmax的技巧，它不能单独使用，要配合greedy search或者beam search做decoding.</p>
</li>
</ol>
<h2 id="Refereces"><a href="#Refereces" class="headerlink" title="Refereces"></a>Refereces</h2><ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/bqw18744018044/article/details/126944119">NLP综述：（三）自然语言生成-NLG</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/bqw18744018044/article/details/126944119">【自然语言处理】【文本生成】Transformers中用于语言生成的不同解码方法</a></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E4%BC%A0%E7%BB%9F%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="JiangYH">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E4%BC%A0%E7%BB%9F%E6%A8%A1%E5%9E%8B/" class="post-title-link" itemprop="url">传统NLP模型学习</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-12-13 22:56:46 / Modified: 22:58:10" itemprop="dateCreated datePublished" datetime="2023-12-13T22:56:46+08:00">2023-12-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li><a href="#word2vec%E6%A8%A1%E5%9E%8B">Word2Vec模型</a><ul>
<li><a href="#%E6%A8%A1%E5%9E%8B%E4%BD%9C%E7%94%A8">模型作用</a></li>
<li><a href="#%E5%AE%9E%E7%8E%B0%E4%B8%8E%E7%BB%86%E8%8A%82">实现与细节</a></li>
<li><a href="#tricks">Tricks</a></li>
<li><a href="#references">References</a></li>
</ul>
</li>
<li><a href="#glove%E6%A8%A1%E5%9E%8B">glove模型</a><ul>
<li><a href="#references-1">References</a></li>
</ul>
</li>
<li><a href="#lstm%E6%A8%A1%E5%9E%8B">LSTM模型</a></li>
<li><a href="#bilstm%E6%A8%A1%E5%9E%8B">BiLSTM模型</a><ul>
<li><a href="#references-2">References</a></li>
</ul>
</li>
<li><a href="#n-gram%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">N-gram语言模型</a><ul>
<li><a href="#refs">Refs</a></li>
</ul>
</li>
<li><a href="#%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B">词袋模型</a><ul>
<li><a href="#refs-1">Refs</a></li>
</ul>
</li>
</ul>
<h1 id="Word2Vec模型"><a href="#Word2Vec模型" class="headerlink" title="Word2Vec模型"></a>Word2Vec模型</h1><h2 id="模型作用"><a href="#模型作用" class="headerlink" title="模型作用"></a>模型作用</h2><p>对词语实现向量化表示，获得词向量。将one-hot表示转换成为稠密向量表示。</p>
<h2 id="实现与细节"><a href="#实现与细节" class="headerlink" title="实现与细节"></a>实现与细节</h2><ul>
<li>具体模型实现</li>
</ul>
<p>skip-gram:一对多映射</p>
<p>cbow:多对一映射</p>
<ul>
<li>实现方式</li>
</ul>
<p>通过词向量的点积计算，获取词与词之间的相似性；并采用softmax映射到概率值。通过梯度下降，调整词向量的表示，进而获得良好的词向量。</p>
<p><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E4%BC%A0%E7%BB%9F%E6%A8%A1%E5%9E%8B/b213f095-4f5b-40e6-a3fa-540de9a85157.png" alt="b213f095-4f5b-40e6-a3fa-540de9a85157"></p>
<p>两个网络权重行向量和列向量分别表示当前的词向量。</p>
<ul>
<li>word2vec训练加速：</li>
</ul>
<p>由于计算概率值时，需要对整个单词表做乘积和exp运算，因此计算量耗费较大。</p>
<p>高频词抽样；负采样提高训练效率；层级softmax</p>
<h2 id="Tricks"><a href="#Tricks" class="headerlink" title="Tricks"></a>Tricks</h2><ul>
<li>负采样</li>
</ul>
<p><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E4%BC%A0%E7%BB%9F%E6%A8%A1%E5%9E%8B/aced802f-f116-450a-8539-e53b6dde5878.png" alt="负采样损失函数"></p>
<p>从单词表中按照一定规则随机选出一部分负样本，再进行概率计算，避免了在整个词汇表上的计算。并将原本的指数计算转换成sigmoid函数，也减小了这部分的计算量。</p>
<p>计算公式：<br>$$<br> \frac{\partial{J_{neg-sample}(\boldsymbol v_c,o,\boldsymbol    U)}}{\partial\boldsymbol  v_c} \&#x3D; \frac{\partial (-log(\sigma (\boldsymbol u_o^T\boldsymbol v_c))-\sum_{k&#x3D;1}^{K} log(\sigma (-\boldsymbol u_k^T\boldsymbol v_c)))}{\partial \boldsymbol v_c} \&#x3D; -\frac{\sigma(\boldsymbol u_o^T\boldsymbol v_c)(1-\sigma(\boldsymbol u_o^T\boldsymbol v_c))}{\sigma(\boldsymbol u_o^T\boldsymbol v_c)}\frac{\partial \boldsymbol u_o^T\boldsymbol v_c}{\partial \boldsymbol v_c} -  \sum_{k&#x3D;1}^{K}\frac{\partial log(\sigma(-\boldsymbol u_k^T\boldsymbol v_c))}{\partial \boldsymbol v_c} \&#x3D; -(1-\sigma(\boldsymbol u_o^T\boldsymbol v_c))\boldsymbol u_o+\sum_{k&#x3D;1}^{K}(1-\sigma(-\boldsymbol u_k^T\boldsymbol v_c))\boldsymbol u_k<br>$$</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26306795">[NLP] 秒懂词向量Word2vec的本质</a></li>
<li><a target="_blank" rel="noopener" href="https://xurydbthjy.feishu.cn/wiki/wikcnsCg5YPVEa4Q7mchrBk3B9f">word2vec</a></li>
</ol>
<h1 id="glove模型"><a href="#glove模型" class="headerlink" title="glove模型"></a>glove模型</h1><blockquote>
<p>GloVe：Global Vectors for Word Representation</p>
</blockquote>
<p>两者最直观的区别在于，word2vec是“predictive”的模型，而GloVe是“count-based”的模型。</p>
<p>相比Word2Vec，GloVe<strong>更容易并行化</strong>，所以对于较大的训练数据，GloVe更快(<strong>大数据集训练更快</strong>)。</p>
<p>GloVe的本质是对共现矩阵进行降维。</p>
<p>实现流程概括：</p>
<ol>
<li>构建共现矩阵</li>
</ol>
<p>根据语料库构建共现矩阵，每个元素代表当前词(行)与其他词共现的次数(在特定窗口大小内)。并且增加了衰减函数计算权重，距离越远权重越小。</p>
<ol start="2">
<li>词向量与共现矩阵的近似关系</li>
</ol>
<p>$$<br>w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} &#x3D; \log(X_{ij})<br>$$</p>
<ol start="3">
<li>构造损失函数</li>
</ol>
<p>$$<br>J &#x3D; \sum_{i,j&#x3D;1}^{V} f(X_{ij})(w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} – \log(X_{ij}) )^2<br>$$<br>需要在原有的基础上增加一个分段函数调节损失。</p>
<h2 id="References-1"><a href="#References-1" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.biaodianfu.com/glove.html">斯坦福大学的词向量工具：GloVe</a></li>
</ol>
<h1 id="LSTM模型"><a href="#LSTM模型" class="headerlink" title="LSTM模型"></a>LSTM模型</h1><h1 id="BiLSTM模型"><a href="#BiLSTM模型" class="headerlink" title="BiLSTM模型"></a>BiLSTM模型</h1><h2 id="References-2"><a href="#References-2" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/47802053">详解BiLSTM及代码实现 - 知乎 (zhihu.com)</a></li>
</ol>
<h1 id="N-gram语言模型"><a href="#N-gram语言模型" class="headerlink" title="N-gram语言模型"></a>N-gram语言模型</h1><p>N-Gram是一种基于<strong>统计语言模型</strong>的算法。它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列。</p>
<p>每一个字节片段称为gram，对所有gram的出现<strong>频度进行统计</strong>，并且按照事先设定好的阈值进行过滤，形成关键gram列表，也就是这个文本的向量特征空间，列表中的每一种gram就是一个特征向量维度。</p>
<p>根据窗口大小不同,每次截取N个词构成的内容,然后依照one-hot表示得到对应的特征向量</p>
<h2 id="Refs"><a href="#Refs" class="headerlink" title="Refs"></a>Refs</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29555001">N-gram提取特征 - 知乎 (zhihu.com)</a></li>
</ol>
<h1 id="词袋模型"><a href="#词袋模型" class="headerlink" title="词袋模型"></a>词袋模型</h1><p>选定文本内一定的词放入词袋，统计词袋内所有词在文本中出现的次数（忽略语法和单词出现的顺序），将其用向量的形式表示出来。</p>
<p>词袋选词:设置一些停用词;对相近词进行词干提取,只将词干放入词袋;词的同类变形,则通过词形还原将同一个特征加入词袋;</p>
<p>通过TF-IDF调整词袋模型得到的词频向量矩阵</p>
<h2 id="Refs-1"><a href="#Refs-1" class="headerlink" title="Refs"></a>Refs</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/HuZihu/p/9576794.html">文本特征提取—词袋模型，TF-IDF模型，N-gram模型（Text Feature Extraction Bag of Words TF-IDF N-gram ） - HuZihu - 博客园 (cnblogs.com)</a></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/prompt%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="JiangYH">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/prompt%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/" class="post-title-link" itemprop="url">prompt基础知识</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-12-13 22:56:46 / Modified: 20:27:18" itemprop="dateCreated datePublished" datetime="2023-12-13T22:56:46+08:00">2023-12-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li><a href="#prompt%E6%9E%84%E5%BB%BA%E5%8E%9F%E5%88%99">prompt构建原则</a><ul>
<li><a href="#%E6%A8%A1%E5%9E%8B%E5%B1%80%E9%99%90">模型局限</a></li>
</ul>
</li>
<li><a href="#prompt%E7%9A%84%E8%BF%AD%E4%BB%A3">prompt的迭代</a></li>
<li><a href="#%E4%B8%8D%E5%90%8C%E4%BB%BB%E5%8A%A1%E7%9A%84prompt">不同任务的prompt</a><ul>
<li><a href="#%E6%96%87%E6%9C%AC%E6%A6%82%E6%8B%AC">文本概括</a></li>
<li><a href="#%E6%8E%A8%E7%90%86">推理</a></li>
<li><a href="#%E6%96%87%E6%9C%AC%E8%BD%AC%E6%8D%A2">文本转换</a></li>
<li><a href="#%E6%96%87%E6%9C%AC%E6%89%A9%E5%B1%95">文本扩展</a></li>
</ul>
</li>
<li><a href="#prompt%E5%AE%9E%E8%B7%B5">prompt实践</a><ul>
<li><a href="#%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA">聊天机器人</a></li>
</ul>
</li>
<li><a href="#prompt%E4%B8%8Einstruct">Prompt与Instruct</a><ul>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
</li>
</ul>
<h1 id="prompt构建原则"><a href="#prompt构建原则" class="headerlink" title="prompt构建原则"></a>prompt构建原则</h1><ol>
<li>清晰准确的指令</li>
</ol>
<ul>
<li>使用分隔符，将不同的内容分隔开来</li>
<li>要求模型给出HTML、JSON等标准化输出</li>
<li>给出当前任务需要的假设条件</li>
<li>给出一些标准样例作为提示</li>
</ul>
<ol start="2">
<li>需要给模型思考的时间(CoT)</li>
</ol>
<ul>
<li>指定完成一个任务需要的具体步骤</li>
<li>需要先引导模型完成自我思考，再进行对比结论总结；直接让其对内容判断会出现一些误判</li>
<li></li>
</ul>
<h2 id="模型局限"><a href="#模型局限" class="headerlink" title="模型局限"></a>模型局限</h2><p>模型幻觉，会生成一些看似合理但不正确的结果。</p>
<p>减轻影响的办法：</p>
<p>先让模型找到相关的知识背景内容；然后再让模型基于这些相关内容回答问题；</p>
<h1 id="prompt的迭代"><a href="#prompt的迭代" class="headerlink" title="prompt的迭代"></a>prompt的迭代</h1><p>流程：</p>
<ol>
<li>给出确定清晰的prompt</li>
<li>针对具体的出现的问题逐步迭代优化</li>
<li>必要时可以对批量样本迭代评估测试prompt性能</li>
</ol>
<p>！prompt本身没有这么重要，主要是需要有能力迭代优化prompt</p>
<h1 id="不同任务的prompt"><a href="#不同任务的prompt" class="headerlink" title="不同任务的prompt"></a>不同任务的prompt</h1><h2 id="文本概括"><a href="#文本概括" class="headerlink" title="文本概括"></a>文本概括</h2><h2 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h2><h2 id="文本转换"><a href="#文本转换" class="headerlink" title="文本转换"></a>文本转换</h2><h2 id="文本扩展"><a href="#文本扩展" class="headerlink" title="文本扩展"></a>文本扩展</h2><h1 id="prompt实践"><a href="#prompt实践" class="headerlink" title="prompt实践"></a>prompt实践</h1><h2 id="聊天机器人"><a href="#聊天机器人" class="headerlink" title="聊天机器人"></a>聊天机器人</h2><h1 id="Prompt与Instruct"><a href="#Prompt与Instruct" class="headerlink" title="Prompt与Instruct"></a>Prompt与Instruct</h1><p>prompt：激发语言模型的补全能力；针对具体的下游任务；</p>
<p>Instruct：激发语言模型的理解能力；设计面向多任务微调，进而实现有效的zero-shot；</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/590311003">ChatGPT&#x2F;InstructGPT详解</a></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/Prompt%E8%8C%83%E5%BC%8F%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="JiangYH">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/Prompt%E8%8C%83%E5%BC%8F%E6%A8%A1%E5%9E%8B/" class="post-title-link" itemprop="url">prompt类预模型学习</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-12-13 22:56:46 / Modified: 22:57:42" itemprop="dateCreated datePublished" datetime="2023-12-13T22:56:46+08:00">2023-12-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li><a href="#uie%E6%A8%A1%E5%9E%8B">UIE模型</a><ul>
<li><a href="#%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF"><strong>研究背景</strong></a></li>
<li><a href="#%E4%B8%BB%E8%A6%81%E5%B7%A5%E4%BD%9C"><strong>主要工作</strong></a></li>
<li><a href="#references">References</a></li>
</ul>
</li>
</ul>
<h1 id="UIE模型"><a href="#UIE模型" class="headerlink" title="UIE模型"></a>UIE模型</h1><!-- 论文名 -->
<blockquote>
<p>UIE: Unified Structure Generation for Universal Information Extraction</p>
</blockquote>
<h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><!-- 研究点与创新点 -->
<p>在信息抽取(IE)领域,由于抽取目标多样、多种不同的复杂异构结构以及领域的需求多变，导致当前IE难以统一建模。极大限制了IE系统高效架构开发、有效知识共享、快速跨域适配。</p>
<p>针对不同任务设定，需要针对特定领域schema建模，不同IE模型被单个训练、不共享，一个公司可能需要管理众多IE模型。</p>
<p>UIE提出生成式统一建模，通过统一文本到结构生成框架实现：</p>
<ul>
<li>统一地建模不同的IE任务；</li>
<li>自适应地生成目标结构；</li>
<li>从不同的知识来源统一学习通用的信息抽取能力</li>
</ul>
<h2 id="主要工作"><a href="#主要工作" class="headerlink" title="主要工作"></a><strong>主要工作</strong></h2><ol>
<li>统一建模：文本到结构生成</li>
</ol>
<p>SSI(结构化模式提示器)：统一prompt编码结构，即编码实体、关系、事件统一表示。</p>
<p>SEL(结构化抽取语言):将不同任务抽取结果统一用一种方式表达。</p>
<p><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/Prompt%E8%8C%83%E5%BC%8F%E6%A8%A1%E5%9E%8B/v2-eaf56dcb4d2d967ba541bc27ef1c41cd_720w.webp" alt="建模方式"></p>
<ol start="2">
<li>预训练与微调</li>
</ol>
<ul>
<li>预训练：大规模异构监督预训练</li>
</ul>
<p>对三种预训练预料数据分别构建了对应的数据结构。并在预训练阶段将这三部分数据训练整合训练。分别构建3个与训练任务，使得模型能够有从文本到结构数据映射的能力，具备SEL的结构化能力以及基础的编码能力</p>
<ul>
<li>微调：拒识噪声注入的模型微调机制</li>
</ul>
<p>随机采样SEL中不存在的SpotName类别和AssoName类别，即：(SPOTNAME, [NULL]) 和 (ASSONAME, [NULL])，学会拒绝生成错误结果的能力，如下图所示：</p>
<p><img src="https://pic4.zhimg.com/80/v2-8d8ab702762b7f7b6a5290c35d0bf443_720w.webp" alt="fintune"></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/495600185">信息抽取大一统：百度中科院发布通用抽取模型UIE，刷新13个IE数据集SOTA！</a></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/PLMs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="JiangYH">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/PLMs/" class="post-title-link" itemprop="url">预训练模型学习</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-12-13 21:17:17 / Modified: 21:58:26" itemprop="dateCreated datePublished" datetime="2023-12-13T21:17:17+08:00">2023-12-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li><a href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B">预训练模型</a><ul>
<li><a href="#%E8%AF%84%E4%BC%B0">评估</a><ul>
<li><a href="#ref">ref</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#deberta%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B">DeBERTa系列模型</a><ul>
<li><a href="#%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF"><strong>研究背景</strong></a></li>
<li><a href="#%E4%B8%BB%E8%A6%81%E5%B7%A5%E4%BD%9C"><strong>主要工作</strong></a></li>
<li><a href="#references">References</a></li>
</ul>
</li>
<li><a href="#%E5%9F%BA%E7%A1%80plm%E7%B3%BB%E5%88%97">基础PLM系列</a><ul>
<li><a href="#%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86">基础知识</a><ul>
<li><a href="#cls%E6%A0%87%E8%AF%86">CLS标识</a><ul>
<li><a href="#references-1">References</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#roberta%E6%A8%A1%E5%9E%8B">RoBERTa模型</a></li>
<li><a href="#xlnet%E6%A8%A1%E5%9E%8B">XLNET模型</a><ul>
<li><a href="#references-2">References</a></li>
</ul>
</li>
<li><a href="#ernie%E6%A8%A1%E5%9E%8B">ERNIE模型</a><ul>
<li><a href="#references-3">References</a></li>
</ul>
</li>
<li><a href="#albert">ALBERT</a><ul>
<li><a href="#references-4">References</a></li>
</ul>
</li>
<li><a href="#electra%E6%A8%A1%E5%9E%8B">ELECTRA模型</a><ul>
<li><a href="#references-5">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="预训练模型"><a href="#预训练模型" class="headerlink" title="预训练模型"></a>预训练模型</h1><h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><p>评估预训练语言模型的优劣通常有以下几种方法：</p>
<p>困惑度（Perplexity）：困惑度是一种常用的评估语言模型的方法，它可以用来衡量模型对新数据的预测能力。困惑度越低，表示模型对数据的拟合效果越好。</p>
<p>语言模型下游任务：语言模型下游任务是指在特定任务上使用预训练语言模型进行微调，以便更好地适应该任务。通常，如果预训练语言模型在下游任务上表现良好，则说明该模型具有较好的泛化能力和语言理解能力。</p>
<p>人类评估：人类评估是指通过人工判断预训练语言模型生成的文本是否符合语法、逻辑和语义等方面的要求。虽然这种方法比较费时费力，但是它可以提供更加客观的评估结果。</p>
<p>对抗样本攻击：对抗样本攻击是指通过对预训练语言模型输入进行修改，使其输出错误结果或误导结果。通过对抗样本攻击，可以评估模型的鲁棒性和安全性。</p>
<p>多样性和一致性：多样性和一致性是指预训练语言模型在生成文本时是否有足够的创造力和一致性。如果模型生成的文本过于单调或者不一致，可能会影响其应用价值。</p>
<p>训练效率和存储空间：除了以上几个方面，评估预训练语言模型的优劣还需要考虑其训练效率和存储空间等因素。一般来说，训练效率和存储空间越小，表示该模型越实用</p>
<h3 id="ref"><a href="#ref" class="headerlink" title="ref"></a>ref</h3><ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/pipisorry/article/details/131165857">https://blog.csdn.net/pipisorry/article/details/131165857</a></li>
</ol>
<h1 id="DeBERTa系列模型"><a href="#DeBERTa系列模型" class="headerlink" title="DeBERTa系列模型"></a>DeBERTa系列模型</h1><!-- 论文名 -->
<blockquote>
<p>DeBERTa: Decoding-enhanced BERT with Disentangled Attention</p>
</blockquote>
<h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><!-- 研究点与创新点 -->

<p><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/PLMs/v2-0d5ce3648f1bfeb9d7bd8f407be6721a_720w.webp" alt="deberta-model"></p>
<p>在Bert的基础上对模型进行改进，取得了不错的效果。</p>
<h2 id="主要工作"><a href="#主要工作" class="headerlink" title="主要工作"></a><strong>主要工作</strong></h2><p>deberta-1.0</p>
<ol>
<li><p>解耦self attention</p>
<ul>
<li><p>Disentangled Attention</p>
<p>  一种新的相对位置编码方法；<br>  这里的解耦是将<strong>位置信息和内容信息</strong>分别&#x2F;交叉做attention，而这里的位置信息在Deberta中采用的是<strong>相对位置编码</strong></p>
</li>
</ul>
<p> <img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/PLMs/v2-c42263e119a1269e65f37cb1530112fa_720w.webp" alt="相对位置计算"><br> <img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/PLMs/image-20230525114212503.png" alt="image-20230525114212503">
 </p>
</li>
<li><p>考虑绝对位置的MLM任务</p>
<ul>
<li>Enhanced Mask Decoder</li>
</ul>
<p> <img src="https://pic3.zhimg.com/80/v2-bbe511611bfac703b014a58e14101f12_720w.webp" alt="EMD"><br>     BERT结构存在<strong>预训练和微调不一致的问题</strong>，即预训练时是将最终的隐状态输入softmax层预测masked tokens，而微调时根据下游任务的不同其结构存在差异。<br>     EMD将模型在预训练时的结构加以改变，其结构如上所示，其中H为之前Transformer层的隐状态，I可以是任何对于decoding有帮助的信息（例如：直接用H，绝对位置信息，之前EMD层的输出等）。<br>     通过信息增加有助于调整需要的特征。</p>
</li>
<li><p>预训练引入对抗训练</p>
<ul>
<li>SIFT（scale invariant fine tuning）</li>
</ul>
<p> 由于词向量的范数在不同的词和模型中有所不同，若模型较大，方差会变得更大，从而导致虚拟对抗训练的不稳定。<br> 所以在首先要对词向量归一化为随机向量，然后再对词向量施加扰动进行虚拟对抗训练。</p>
</li>
</ol>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/505440976">还在用RoBERTa？快来看看DeBERTa吧！</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/522166837">预训练模型–DeBERTa</a></li>
</ol>
<h1 id="基础PLM系列"><a href="#基础PLM系列" class="headerlink" title="基础PLM系列"></a>基础PLM系列</h1><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="CLS标识"><a href="#CLS标识" class="headerlink" title="CLS标识"></a>CLS标识</h3><h4 id="References-1"><a href="#References-1" class="headerlink" title="References"></a>References</h4><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/360343071">关于BERT中的那些为什么</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/643560888">大模型面试八股</a></li>
<li></li>
</ol>
<h2 id="RoBERTa模型"><a href="#RoBERTa模型" class="headerlink" title="RoBERTa模型"></a>RoBERTa模型</h2><ul>
<li>核心思想</li>
</ul>
<p>通过更好地训练BERT可以达到超过其他新的预训练语言模型的效果</p>
<ul>
<li>核心改动</li>
</ul>
<ol>
<li>更大的 Batch Size （最大的 Batch Size 达到了 32K）</li>
<li>去掉 Next Sentence Prediction （在建模时需要注意这一点）</li>
<li>采用更大的预训练语料 （超过100G）</li>
<li>Dynamic Masking （BERT在训练时可能会固定地把一个地方 Mask几遍）</li>
</ol>
<h2 id="XLNET模型"><a href="#XLNET模型" class="headerlink" title="XLNET模型"></a>XLNET模型</h2><ul>
<li>研究背景</li>
</ul>
<p>Bert采用AE（自编码）方法存在的问题：</p>
<ol>
<li>有个不符合真实情况的假设：即被mask掉的token是相互独立的。</li>
<li>BERT在预训练和精调阶段存在差异</li>
</ol>
<ul>
<li>改进方案</li>
</ul>
<ol>
<li>对序列重新组合，让模型能够学习如何聚集所有位置的信息，Permutation Language Modeling Transformer-XL，主要用于解决长文本的问题</li>
<li>Two-Stream Self-Attention，由于前面的排序重组会导致同一序列不知道预测什么内容的情况，为了解决这一问题模型加入了位置信息。</li>
<li>借鉴RNN，提出带有记忆能力的Transformer-XL<br><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/PLMs/c86f0ea2-5a70-4d42-ab29-b1a5fc933bcd.png" alt="Transformer-XL"></li>
<li>增加了多片段建模的方法，判断两个token是否在一个片段中。具体是在计算注意力权重的同时针对query额外计算一个权重，加到原本的权重上去。</li>
<li>增大了预训练阶段使用数据的规模</li>
</ol>
<h3 id="References-2"><a href="#References-2" class="headerlink" title="References"></a>References</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/70257427">张俊林：XLNet:运行机制及和Bert的异同比较</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/330307904/answer/721986216">如何评价在20个任务上超越BERT的XLNet？</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/70218096">李rumor：Google XLNet原理解读</a></li>
</ol>
<h2 id="ERNIE模型"><a href="#ERNIE模型" class="headerlink" title="ERNIE模型"></a>ERNIE模型</h2><ul>
<li>ERNIE 1.0</li>
</ul>
<p>改进了masking的策略</p>
<ol>
<li>基于短语的</li>
<li>基于实体的</li>
</ol>
<ul>
<li>ERNIE 2.0</li>
</ul>
<blockquote>
<p>核心：提出了一个预训练框架，可以在大型数据集上进行增量训练</p>
</blockquote>
<ol>
<li>预训练连续学习，能够在学习新的任务的时候记住之前任务的结果。具体实现，当前工作分别构建了词法级别、语法级别和语义级别的预训练任务</li>
<li>encoder权重不共享</li>
<li>用不同的task id标记预训练任务</li>
</ol>
<h3 id="References-3"><a href="#References-3" class="headerlink" title="References"></a>References</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/87008569">ModifyAI：一文读懂最强中文NLP预训练模型ERNIE</a></li>
</ol>
<h2 id="ALBERT"><a href="#ALBERT" class="headerlink" title="ALBERT"></a>ALBERT</h2><ul>
<li>核心思想：</li>
</ul>
<blockquote>
<p>权重共享 输入层的优化 Sentence Order Prediction </p>
</blockquote>
<ul>
<li>工作与总结：</li>
</ul>
<p>ALBERT的核心思想是采用了两种减少模型参数的方法，比BERT占用的内存空间小很多，同时极大提升了训练速度，更重要的是效果上也有很大的提升！</p>
<ul>
<li>具体工作细节：</li>
</ul>
<ol>
<li><p>Factorized Embedding Parameterization</p>
<p> 原Bert-base由12层Transformer中的encoder组成，经由bert获得的向量表示维度H与其一开始的Embedding层维度E一致，但是其实没有必要，E大小可以根据实际的词表大小调节，此时若要保持H维度大小的输出仅需E*H的变换即可。</p>
</li>
<li><p>Cross-layer Parameter Sharing</p>
<p> 共享所有层的参数，主要是attention和FeedForward参数，该手段则是通过共享部分attention和Feedforward参数实现参数量的减少，此时效果会有所下降，但通过增加H的维度实现效果提升-推理速度不变</p>
</li>
<li><p>Sentence Order Prediction</p>
<p> NSP预训练任务将Topic Prediction和Coherence prediction融合起来了，只要判断两个句子是不是一个Topic的就能对预训练任务出个大概的结果了。论文通过将负样本换成同一篇文章中的两个逆序句子，来消除Topic prediction，提升预训练任务的学习效果。</p>
</li>
</ol>
<p>参数量以及具体效果分析：</p>
<p>bert-base:108M<br>albert-base:89M(no-shared),12M(shared)</p>
<p>参数量减少了，但是并没有对模型推理速度这一块有较大的提升。主要还是减少了模型的参数量加快模型的训练，并没有对推理有太好的效果提升。</p>
<h3 id="References-4"><a href="#References-4" class="headerlink" title="References"></a>References</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/268130746">Mr.robot：面试中理解ALBERT？（NLP面经）</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/485441585/answer/2112050954">ALBERT 如何有效减少 BERT 的参数？</a></li>
</ol>
<h2 id="ELECTRA模型"><a href="#ELECTRA模型" class="headerlink" title="ELECTRA模型"></a>ELECTRA模型</h2><ul>
<li>核心思想：采用对抗训练提升模型训练效果</li>
<li>具体实现：通过 MLM 训练 Generator Discriminator 负责区分 Generator 生成的 token 是否被替代</li>
<li>其他改进：采用权重共享</li>
</ul>
<h3 id="References-5"><a href="#References-5" class="headerlink" title="References"></a>References</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/89763176">李rumor：ELECTRA: 超越BERT, 19年最佳NLP预训练模型</a></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/CV%E6%96%B9%E5%90%91/vit%E7%B3%BB%E5%88%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="JiangYH">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/CV%E6%96%B9%E5%90%91/vit%E7%B3%BB%E5%88%97/" class="post-title-link" itemprop="url">ViT系列模型学习</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-12-13 21:14:32 / Modified: 22:59:54" itemprop="dateCreated datePublished" datetime="2023-12-13T21:14:32+08:00">2023-12-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="ViT模型"><a href="#ViT模型" class="headerlink" title="ViT模型"></a>ViT模型</h1><!-- 论文名 -->
<blockquote>
<p>ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</p>
</blockquote>
<h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><!-- 研究点与创新点 -->
<p>ViT是2020年Google团队提出的将Transformer应用在图像分类的模型，虽然不是第一篇将transformer应用在视觉任务的论文，但是因为其模型“简单”且效果好，可扩展性强（scalable，模型越大效果越好），成为了transformer在CV领域应用的里程碑著作，也引爆了后续相关研究.</p>
<p>ViT原论文中最核心的结论是，当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果.</p>
<p>但是当训练数据集不够大的时候，ViT的表现通常比同等大小的ResNets要差一些，因为<strong>Transformer和CNN相比缺少归纳偏置</strong>（inductive bias），即一种先验知识，提前做好的假设。</p>
<p>CNN具有两种归纳偏置，一种是局部性（locality&#x2F;two-dimensional neighborhood structure），即图片上相邻的区域具有相似的特征；一种是平移不变形（translation equivariance）.当CNN具有以上两种归纳偏置，就有了很多先验信息，需要相对少的数据就可以学习一个比较好的模型</p>
<p><img src="https://pic3.zhimg.com/v2-11c956744a01c97c1da1cf91612b8e3e_r.jpg" alt="VIT-model"></p>
<p><img src="https://pic2.zhimg.com/80/v2-54f717f71079becca62a0247660a171d_720w.webp" alt="ViT-param"></p>
<h2 id="主要工作"><a href="#主要工作" class="headerlink" title="主要工作"></a><strong>主要工作</strong></h2><ol>
<li>输入：将图像拆成N个$p*p$的小patch，然后将每个patch当成是一个token；</li>
<li>具体到每个patch则是直接进行flatten之后直接经过线性映射得到D维Embedding表示；</li>
<li>这里的pos-embedding按照实验是直接采用1-D位置编码,按照从左到右的块排序</li>
</ol>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/438883618">Vision Transformer学习笔记1：ViT</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/445122996">ViT（Vision Transformer）解析</a></li>
</ol>
<hr>
<h1 id="MLP-Mixer"><a href="#MLP-Mixer" class="headerlink" title="MLP-Mixer"></a>MLP-Mixer</h1><!-- 论文名 -->
<blockquote>
<p>MLP-Mixer: MLP-Mixer: An all-MLP Architecture for Vision</p>
</blockquote>
<h2 id="研究背景-1"><a href="#研究背景-1" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><!-- 研究点与创新点 -->
<p>ViT作者团队出品，在CNN和Transformer大火的背景下，舍弃了卷积和注意力机制，提出了MLP-Mixer，一个完全基于MLPs的结构，其MLPs有两种类型，分别是<strong>channel-mixing MLPs</strong>和<strong>token-mixing MLPs</strong>，前者独立作用于image patches（融合通道信息），后者跨image patches作用（融合空间信息）。<br><img src="https://pic3.zhimg.com/80/v2-ed96c7a5add85b9151c7f10fbda4943a_720w.webp" alt="mlp-mixer-model"></p>
<h2 id="主要工作-1"><a href="#主要工作-1" class="headerlink" title="主要工作"></a><strong>主要工作</strong></h2><ol>
<li>类似于ViT的模型结构训练方式,也需要将图像信息打成多个块;</li>
<li>提出两种MLPs结构<br><code>token-mixing MLPs</code>：允许信息在空间维度交互，独立作用于每一个channel，作用于列，融合不同token的特征<br><code>channel-mixing MLPs</code>：允许信息在通道交互，独立作用于每一个token，作用于行，融合不同channel的特征<br><img src="https://pic2.zhimg.com/80/v2-58df6c737fdea377071868f106b7c1c1_720w.webp" alt="mlp-layer"></li>
<li>因为token-mixing MLPs对输入tokens的顺序非常敏感,Mixer不适用positional encoding</li>
<li>每个Mixer Layer中token-mixing MLPs共享参数，channel-mixing MLPs同样共享参数</li>
<li>当在大规模数据集上预训练（100million images），Mixer可以接近CNNs和Transformers的SOTA表现，在ImageNet上达到87.94%的top-1 accuracy；当在更小规模数据集上预训练时（10million），结合一些regularization techniques，Mixer可以接近ViT的性能，但是稍逊于CNN</li>
</ol>
<h2 id="References-1"><a href="#References-1" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/445122996">ViT（Vision Transformer）解析</a></li>
</ol>
<hr>
<h1 id="Swin-Transformer模型"><a href="#Swin-Transformer模型" class="headerlink" title="Swin-Transformer模型"></a>Swin-Transformer模型</h1><!-- 论文名 -->
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.14030.pdf">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></p>
</blockquote>
<h2 id="研究背景-2"><a href="#研究背景-2" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><!-- 研究点与创新点 -->
<p><img src="https://pic3.zhimg.com/v2-9a475a9b8389c48ea61da8f0b821fe56_r.jpg" alt="swin-transformer-model"></p>
<ul>
<li>核心思路：披着CNN的Transformer。</li>
<li>挑战：ViT性能并没有超过其他的工作；基于全局自注意力计算会导致计算量较大;强行分割patch其实破坏了原有的邻域结构，不再具有卷积的空间不变性。</li>
<li>通过提出一种称为shifted window的方法来解决以上问题。</li>
</ul>
<h2 id="主要工作-2"><a href="#主要工作-2" class="headerlink" title="主要工作"></a><strong>主要工作</strong></h2><ol>
<li><p>结构大致介绍：</p>
<ul>
<li>在输入开始的时候，做了一个Patch Embedding，将图片切成一个个图块，并嵌入到Embedding。</li>
<li>在每个Stage里，由Patch Merging和多个Block组成。</li>
<li>其中Patch Merging模块主要在每个Stage一开始降低图片分辨率。</li>
<li>而Block具体结构如上图所示，主要是LayerNorm，MLP，Window Attention 和 Shifted Window Attention组成</li>
</ul>
</li>
<li><p>Patch Embedding</p>
<pre><code> 将图片划分为若干4*4的patch，使用线性变换来将patch变为Embedding向量，这一步和ViT是一样的。但是注意，这里的patch比ViT的14*14小了很多。
</code></pre>
</li>
<li><p>Patch Merging</p>
<pre><code> 该模块的作用是在每个Stage开始前做降采样，用于缩小分辨率，调整通道数 进而形成层次化的设计，同时也能节省一定运算量。每次降采样是两倍，因此在行方向和列方向上，间隔2选取元素。
</code></pre>
</li>
<li><p>Window Attention</p>
<pre><code> 传统的Transformer都是基于全局来计算注意力的，因此计算复杂度十分高。而Swin Transformer则将注意力的计算限制在每个窗口内，进而减少了计算量.
 主要计算区别在于：在原始计算Attention的公式中的Q,K时加入了相对位置编码。
</code></pre>
</li>
<li><p>Shifted Window Attention</p>
</li>
</ol>
<p><img src="https://pic1.zhimg.com/80/v2-07a98325a29db1da6521e4ddaaed3c88_720w.webp" alt="swa"></p>
<pre><code>    前面的Window Attention是在每个窗口下计算注意力的，为了更好的和其他window进行信息交互，Swin Transformer还引入了shifted window操作。
    由于这一操作会使得window变化，因此实际操作中通过对特征图移位，并给Attention设置mask来间接实现的。能在保持原有的window个数下，最后的计算结果等价。
</code></pre>
<h2 id="References-2"><a href="#References-2" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/367111046">图解Swin Transformer</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/442006157">ViT学习笔记2：Swin Transformer</a></li>
</ol>
<h1 id="Swin-Transformer-v2模型"><a href="#Swin-Transformer-v2模型" class="headerlink" title="Swin-Transformer_v2模型"></a>Swin-Transformer_v2模型</h1><p>主要是解决模型上规模的问题，有几个主要的改动：</p>
<ol>
<li>把每个Block里的LN从前面换到了后面，来解决深度增加之后训练不稳定的问题</li>
<li>把原来的scaled dot attention换成了scaled cosine attention，也是为了解决训练不稳定的问题（否则可能被某些像素对的相似度主导）。</li>
<li>改进相对位置偏置。V1版里这个模块是用一个规模跟窗口大小M相关可学习参数矩阵来处理的，如果预训练和finetune时M大小改变，就用插值来生成原来不存在的值。V2版首先是引入了一个小网络来取代参数矩阵，其次是将相对位置从线性空间换到了对数空间，通过取对数压缩空间差距来让M变化时的过渡更加顺滑</li>
</ol>
<h2 id="References-3"><a href="#References-3" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/442006157">ViT学习笔记2：Swin Transformer</a></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/CV%E6%96%B9%E5%90%91/CV%E5%9F%BA%E7%A1%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="JiangYH">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/CV%E6%96%B9%E5%90%91/CV%E5%9F%BA%E7%A1%80/" class="post-title-link" itemprop="url">CV基础</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-12-13 21:14:32 / Modified: 23:02:21" itemprop="dateCreated datePublished" datetime="2023-12-13T21:14:32+08:00">2023-12-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="CV中的特征维度变换"><a href="#CV中的特征维度变换" class="headerlink" title="CV中的特征维度变换"></a>CV中的特征维度变换</h1><p>CNN的卷积核通道数 &#x3D; 卷积输入层的通道数<br>CNN的卷积输出层通道数(深度)&#x3D; 卷积核的个数</p>
<p>设输入图像尺寸为WxW，卷积核尺寸为FxF，步幅为S，Padding使用P,经过该卷积层后输出的图像尺寸为NxN:$N&#x3D;\frac{W-F+2P}{S}+1$</p>
<p>池化计算：</p>
<p>$W&#x3D;\frac{W-F}{S}+1,H&#x3D;\frac{H-F}{S}+1$</p>
<h2 id="空洞卷积变换计算"><a href="#空洞卷积变换计算" class="headerlink" title="空洞卷积变换计算"></a>空洞卷积变换计算</h2><p>$W&#x3D;\frac{W-d(F-1)+2p}{S}+1$,d为空洞率，正常卷积为1，F为卷积核大小</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">JiangYH</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
