<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">




<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

  <meta name="author" content="JiangYH">





<title>预训练大模型学习 | 通往成功的道路</title>



<link rel="icon" href="/favicon.png">



<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/nprogress/nprogress.css">



<script src="/lib/jquery.min.js"></script>


<script src="/lib/iconify-icon.min.js"></script>


<script src="https://cdn.tailwindcss.com?plugins=typography"></script>
<script>
  tailwind.config = {
    darkMode: "class",
  };
</script>


<script src="/lib/nprogress/nprogress.js"></script>

<script>
  $(document).ready(() => {
    NProgress.configure({
      showSpinner: false,
    });
    NProgress.start();
    $("#nprogress .bar").css({
      background: "#de7441",
    });
    $("#nprogress .peg").css({
      "box-shadow": "0 0 2px #de7441, 0 0 4px #de7441",
    });
    $("#nprogress .spinner-icon").css({
      "border-top-color": "#de7441",
      "border-left-color": "#de7441",
    });
    setTimeout(function () {
      NProgress.done();
      $(".fade").removeClass("out");
    }, 800);
  });
</script>

<script>
  (function () {
    const prefersDark =
      window.matchMedia &&
      window.matchMedia("(prefers-color-scheme: dark)").matches;
    const setting = localStorage.getItem("hexo-color-scheme") || "auto";
    if (setting === "dark" || (prefersDark && setting !== "light"))
      document.documentElement.classList.toggle("dark", true);
    let isDark = document.documentElement.classList.contains("dark");
  })();

  $(document).ready(function () {
    // init icon
    const prefersDark =
      window.matchMedia &&
      window.matchMedia("(prefers-color-scheme: dark)").matches;
    const isDark = document.documentElement.classList.contains("dark");
    $("#theme-icon").attr("icon", isDark ? "ri:moon-line" : "ri:sun-line");

    function toggleGiscusTheme() {
      const isDark = document.documentElement.classList.contains("dark");
      const giscusFrame = document.querySelector("iframe.giscus-frame");
      if (giscusFrame) {
        giscusFrame.contentWindow.postMessage(
          {
            giscus: {
              setConfig: {
                theme: isDark ? "dark" : "light",
              },
            },
          },
          "https://giscus.app"
        );
      }
    }


    // toggle dark mode
    function toggleDark() {
      let isDark = document.documentElement.classList.contains("dark");
      const setting = localStorage.getItem("hexo-color-scheme") || "auto";
      isDark = !isDark;
      document.documentElement.classList.toggle("dark", isDark);
      $("#theme-icon").attr("icon", isDark ? "ri:moon-line" : "ri:sun-line");
      if (prefersDark === isDark) {
        localStorage.setItem("hexo-color-scheme", "auto");
      } else {
        localStorage.setItem("hexo-color-scheme", isDark ? "dark" : "light");
      }
      toggleGiscusTheme();
    }
    $("#toggle-dark").click(toggleDark);

    // listen dark mode change
    window
      .matchMedia("(prefers-color-scheme: dark)")
      .addEventListener("change", (e) => {
        const setting = localStorage.getItem("hexo-color-scheme") || "auto";
        if (setting === "auto") {
          document.documentElement.classList.toggle("dark", e.matches);
          $("#theme-icon").attr(
            "icon",
            e.matches ? "ri:moon-line" : "ri:sun-line"
          );
          toggleGiscusTheme();
        }
      });
  });
</script>




<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head>
<body class="font-sans bg-white dark:bg-zinc-900 text-gray-700 dark:text-gray-200 relative">
  <header class="fixed w-full px-5 py-1 z-10 backdrop-blur-xl backdrop-saturate-150 border-b border-black/5">
  <div class="max-auto">
    <nav class="flex items-center text-base">
      <a href="/" class="group">
        <h2 class="font-medium tracking-tighterp text-l p-2">
          <img class="w-5 mr-2 inline-block transition-transform group-hover:rotate-[30deg]" id="logo" src="/images/logo.svg" alt="通往成功的道路" />
          通往成功的道路
        </h2>
      </a>
      <div id="header-title" class="opacity-0 md:ml-2 md:mt-[0.1rem] text-xs font-medium whitespace-nowrap overflow-hidden overflow-ellipsis">
        预训练大模型学习
      </div>
      <div class="flex-1"></div>
      <div class="flex items-center gap-3">
        
          <a class="hidden sm:flex" href="/archives">Posts</a>
        
          <a class="hidden sm:flex" href="/category">Categories</a>
        
          <a class="hidden sm:flex" href="/tag">Tags</a>
        
        
          
            <a class="w-5 h-5 hidden sm:flex" title="Github" target="_blank" rel="noopener" href="https://github.com/xbmlz">
              <iconify-icon width="20" icon="ri:github-line"></iconify-icon>
            </a>
          
        
        <a class="w-5 h-5 hidden sm:flex" title="Github" href="rss2.xml">
          <iconify-icon width="20" icon="ri:rss-line"></iconify-icon>
        </a>
        <a class="w-5 h-5" title="toggle theme" id="toggle-dark">
          <iconify-icon width="20" icon="" id="theme-icon"></iconify-icon>
        </a>
      </div>
      <div class="flex items-center justify-center gap-3 ml-3 sm:hidden">
        <span class="w-5 h-5" aria-hidden="true" role="img" id="open-menu">
          <iconify-icon width="20" icon="carbon:menu" ></iconify-icon>
        </span>
        <span class="w-5 h-5 hidden" aria-hidden="true" role="img" id="close-menu">
          <iconify-icon  width="20" icon="carbon:close" ></iconify-icon>
        </span>
      </div>
    </nav>
  </div>
</header>
<div id="menu-panel" class="h-0 overflow-hidden sm:hidden fixed left-0 right-0 top-12 bottom-0 z-10">
  <div id="menu-content" class="relative z-20 bg-white/80 px-6 sm:px-8 py-2 backdrop-blur-xl -translate-y-full transition-transform duration-300">
    <ul class="nav flex flex-col sm:flex-row text-sm font-medium">
      
        <li class="nav-portfolio sm:mx-2 border-b sm:border-0 border-black/5 last:border-0 hover:text-main">
          <a href="/archives" class="flex h-12 sm:h-auto items-center">Posts</a>
        </li>
      
        <li class="nav-portfolio sm:mx-2 border-b sm:border-0 border-black/5 last:border-0 hover:text-main">
          <a href="/category" class="flex h-12 sm:h-auto items-center">Categories</a>
        </li>
      
        <li class="nav-portfolio sm:mx-2 border-b sm:border-0 border-black/5 last:border-0 hover:text-main">
          <a href="/tag" class="flex h-12 sm:h-auto items-center">Tags</a>
        </li>
      
    </ul>
  </div>
  <div class="mask bg-black/20 absolute inset-0"></div>
</div>

  <main class="pt-14">
    <!-- css -->

<link rel="stylesheet" href="/lib/fancybox/fancybox.min.css">


<link rel="stylesheet" href="/lib/tocbot/tocbot.min.css">

<!-- toc -->

  <!-- tocbot -->
<nav class="post-toc toc text-sm w-48 relative top-32 right-0 opacity-70 hidden lg:block" style="position: fixed !important;"></nav>


<section class="px-6 max-w-prose mx-auto md:px-0">
  <!-- header -->
  <header class="overflow-hidden pt-6 pb-6 md:pt-12">
    <div class="pt-4 md:pt-6">
      <h1 id="article-title" class="text-[2rem] font-bold leading-snug mb-4 md:mb-6 md:text-[2.6rem]">
        预训练大模型学习
      </h1>
      <div>
        <section class="flex items-center gap-3 text-sm">
          <span class="flex items-center gap-1">
            <iconify-icon width="18" icon="carbon-calendar" ></iconify-icon>
            <time>2023-09-15</time>
          </span>
          <span class="text-gray-400">·</span>
          <span class="flex items-center gap-1">
            <iconify-icon width="18" icon="ic:round-access-alarm" ></iconify-icon>
            <span>5 min</span>
          </span>
          <span class="text-gray-400">·</span>
          <span class="flex items-center gap-1">
            <iconify-icon width="18" icon="icon-park-outline:font-search" ></iconify-icon>
            <span>1.5k words</span>
          </span>
          
            <span class="text-gray-400">·</span>
            <span class="flex items-center gap-1">
              <iconify-icon width="16" icon="icon-park-outline:box" class="mr-2"></iconify-icon>
              <a class="article-category-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a>
            </span>
          
        </section>
      </div>
    </div>
  </header>
  <!-- content -->
  <article class="post-content prose m-auto slide-enter-content dark:prose-invert">
    <ul>
<li><a href="#deberta%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B">DeBERTa系列模型</a><ul>
<li><a href="#%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF"><strong>研究背景</strong></a></li>
<li><a href="#%E4%B8%BB%E8%A6%81%E5%B7%A5%E4%BD%9C"><strong>主要工作</strong></a></li>
<li><a href="#references">References</a></li>
</ul>
</li>
<li><a href="#%E5%9F%BA%E7%A1%80plm%E7%B3%BB%E5%88%97">基础PLM系列</a><ul>
<li><a href="#%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86">基础知识</a><ul>
<li><a href="#cls%E6%A0%87%E8%AF%86">CLS标识</a><ul>
<li><a href="#references-1">References</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#roberta%E6%A8%A1%E5%9E%8B">RoBERTa模型</a></li>
<li><a href="#xlnet%E6%A8%A1%E5%9E%8B">XLNET模型</a><ul>
<li><a href="#references-2">References</a></li>
</ul>
</li>
<li><a href="#ernie%E6%A8%A1%E5%9E%8B">ERNIE模型</a><ul>
<li><a href="#references-3">References</a></li>
</ul>
</li>
<li><a href="#albert">ALBERT</a><ul>
<li><a href="#references-4">References</a></li>
</ul>
</li>
<li><a href="#electra%E6%A8%A1%E5%9E%8B">ELECTRA模型</a><ul>
<li><a href="#references-5">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="DeBERTa系列模型"><a href="#DeBERTa系列模型" class="headerlink" title="DeBERTa系列模型"></a>DeBERTa系列模型</h1><!-- 论文名 -->
<blockquote>
<p>DeBERTa: Decoding-enhanced BERT with Disentangled Attention</p>
</blockquote>
<h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a><strong>研究背景</strong></h2><!-- 研究点与创新点 -->

<p><img src="/PLMs.assets/v2-0d5ce3648f1bfeb9d7bd8f407be6721a_720w.webp" alt="deberta-model"></p>
<p>在Bert的基础上对模型进行改进，取得了不错的效果。</p>
<h2 id="主要工作"><a href="#主要工作" class="headerlink" title="主要工作"></a><strong>主要工作</strong></h2><p>deberta-1.0</p>
<ol>
<li><p>解耦self attention</p>
<ul>
<li><p>Disentangled Attention</p>
<p>  一种新的相对位置编码方法；<br>  这里的解耦是将<strong>位置信息和内容信息</strong>分别&#x2F;交叉做attention，而这里的位置信息在Deberta中采用的是<strong>相对位置编码</strong></p>
</li>
</ul>
<p> <img src="/PLMs.assets/v2-c42263e119a1269e65f37cb1530112fa_720w.webp" alt="相对位置计算"><br> <img src="/PLMs.assets/image-20230525114212503.png" alt="image-20230525114212503">
 </p>
</li>
<li><p>考虑绝对位置的MLM任务</p>
<ul>
<li>Enhanced Mask Decoder</li>
</ul>
<p> <img src="https://pic3.zhimg.com/80/v2-bbe511611bfac703b014a58e14101f12_720w.webp" alt="EMD"><br>     BERT结构存在<strong>预训练和微调不一致的问题</strong>，即预训练时是将最终的隐状态输入softmax层预测masked tokens，而微调时根据下游任务的不同其结构存在差异。<br>     EMD将模型在预训练时的结构加以改变，其结构如上所示，其中H为之前Transformer层的隐状态，I可以是任何对于decoding有帮助的信息（例如：直接用H，绝对位置信息，之前EMD层的输出等）。<br>     通过信息增加有助于调整需要的特征。</p>
</li>
<li><p>预训练引入对抗训练</p>
<ul>
<li>SIFT（scale invariant fine tuning）</li>
</ul>
<p> 由于词向量的范数在不同的词和模型中有所不同，若模型较大，方差会变得更大，从而导致虚拟对抗训练的不稳定。<br> 所以在首先要对词向量归一化为随机向量，然后再对词向量施加扰动进行虚拟对抗训练。</p>
</li>
</ol>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/505440976">还在用RoBERTa？快来看看DeBERTa吧！</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/522166837">预训练模型–DeBERTa</a></li>
</ol>
<h1 id="基础PLM系列"><a href="#基础PLM系列" class="headerlink" title="基础PLM系列"></a>基础PLM系列</h1><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="CLS标识"><a href="#CLS标识" class="headerlink" title="CLS标识"></a>CLS标识</h3><h4 id="References-1"><a href="#References-1" class="headerlink" title="References"></a>References</h4><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/360343071">关于BERT中的那些为什么</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/643560888">大模型面试八股</a></li>
<li></li>
</ol>
<h2 id="RoBERTa模型"><a href="#RoBERTa模型" class="headerlink" title="RoBERTa模型"></a>RoBERTa模型</h2><ul>
<li>核心思想</li>
</ul>
<p>通过更好地训练BERT可以达到超过其他新的预训练语言模型的效果</p>
<ul>
<li>核心改动</li>
</ul>
<ol>
<li>更大的 Batch Size （最大的 Batch Size 达到了 32K）</li>
<li>去掉 Next Sentence Prediction （在建模时需要注意这一点）</li>
<li>采用更大的预训练语料 （超过100G）</li>
<li>Dynamic Masking （BERT在训练时可能会固定地把一个地方 Mask几遍）</li>
</ol>
<h2 id="XLNET模型"><a href="#XLNET模型" class="headerlink" title="XLNET模型"></a>XLNET模型</h2><ul>
<li>研究背景</li>
</ul>
<p>Bert采用AE（自编码）方法存在的问题：</p>
<ol>
<li>有个不符合真实情况的假设：即被mask掉的token是相互独立的。</li>
<li>BERT在预训练和精调阶段存在差异</li>
</ol>
<ul>
<li>改进方案</li>
</ul>
<ol>
<li>对序列重新组合，让模型能够学习如何聚集所有位置的信息，Permutation Language Modeling Transformer-XL，主要用于解决长文本的问题</li>
<li>Two-Stream Self-Attention，由于前面的排序重组会导致同一序列不知道预测什么内容的情况，为了解决这一问题模型加入了位置信息。</li>
<li>借鉴RNN，提出带有记忆能力的Transformer-XL<br><img src="/Bert%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/c86f0ea2-5a70-4d42-ab29-b1a5fc933bcd.png" alt="Transformer-XL"></li>
<li>增加了多片段建模的方法，判断两个token是否在一个片段中。具体是在计算注意力权重的同时针对query额外计算一个权重，加到原本的权重上去。</li>
<li>增大了预训练阶段使用数据的规模</li>
</ol>
<h3 id="References-2"><a href="#References-2" class="headerlink" title="References"></a>References</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/70257427">张俊林：XLNet:运行机制及和Bert的异同比较</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/330307904/answer/721986216">如何评价在20个任务上超越BERT的XLNet？</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/70218096">李rumor：Google XLNet原理解读</a></li>
</ol>
<h2 id="ERNIE模型"><a href="#ERNIE模型" class="headerlink" title="ERNIE模型"></a>ERNIE模型</h2><ul>
<li>ERNIE 1.0</li>
</ul>
<p>改进了masking的策略</p>
<ol>
<li>基于短语的</li>
<li>基于实体的</li>
</ol>
<ul>
<li>ERNIE 2.0</li>
</ul>
<blockquote>
<p>核心：提出了一个预训练框架，可以在大型数据集上进行增量训练</p>
</blockquote>
<ol>
<li>预训练连续学习，能够在学习新的任务的时候记住之前任务的结果。具体实现，当前工作分别构建了词法级别、语法级别和语义级别的预训练任务</li>
<li>encoder权重不共享</li>
<li>用不同的task id标记预训练任务</li>
</ol>
<h3 id="References-3"><a href="#References-3" class="headerlink" title="References"></a>References</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/87008569">ModifyAI：一文读懂最强中文NLP预训练模型ERNIE</a></li>
</ol>
<h2 id="ALBERT"><a href="#ALBERT" class="headerlink" title="ALBERT"></a>ALBERT</h2><ul>
<li>核心思想：</li>
</ul>
<blockquote>
<p>权重共享 输入层的优化 Sentence Order Prediction </p>
</blockquote>
<ul>
<li>工作与总结：</li>
</ul>
<p>ALBERT的核心思想是采用了两种减少模型参数的方法，比BERT占用的内存空间小很多，同时极大提升了训练速度，更重要的是效果上也有很大的提升！</p>
<ul>
<li>具体工作细节：</li>
</ul>
<ol>
<li><p>Factorized Embedding Parameterization</p>
<p> 原Bert-base由12层Transformer中的encoder组成，经由bert获得的向量表示维度H与其一开始的Embedding层维度E一致，但是其实没有必要，E大小可以根据实际的词表大小调节，此时若要保持H维度大小的输出仅需E*H的变换即可。</p>
</li>
<li><p>Cross-layer Parameter Sharing</p>
<p> 共享所有层的参数，主要是attention和FeedForward参数，该手段则是通过共享部分attention和Feedforward参数实现参数量的减少，此时效果会有所下降，但通过增加H的维度实现效果提升-推理速度不变</p>
</li>
<li><p>Sentence Order Prediction</p>
<p> NSP预训练任务将Topic Prediction和Coherence prediction融合起来了，只要判断两个句子是不是一个Topic的就能对预训练任务出个大概的结果了。论文通过将负样本换成同一篇文章中的两个逆序句子，来消除Topic prediction，提升预训练任务的学习效果。</p>
</li>
</ol>
<p>参数量以及具体效果分析：</p>
<p>bert-base:108M<br>albert-base:89M(no-shared),12M(shared)</p>
<p>参数量减少了，但是并没有对模型推理速度这一块有较大的提升。主要还是减少了模型的参数量加快模型的训练，并没有对推理有太好的效果提升。</p>
<h3 id="References-4"><a href="#References-4" class="headerlink" title="References"></a>References</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/268130746">Mr.robot：面试中理解ALBERT？（NLP面经）</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/485441585/answer/2112050954">ALBERT 如何有效减少 BERT 的参数？</a></li>
</ol>
<h2 id="ELECTRA模型"><a href="#ELECTRA模型" class="headerlink" title="ELECTRA模型"></a>ELECTRA模型</h2><ul>
<li>核心思想：采用对抗训练提升模型训练效果</li>
<li>具体实现：通过 MLM 训练 Generator Discriminator 负责区分 Generator 生成的 token 是否被替代</li>
<li>其他改进：采用权重共享</li>
</ul>
<h3 id="References-5"><a href="#References-5" class="headerlink" title="References"></a>References</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/89763176">李rumor：ELECTRA: 超越BERT, 19年最佳NLP预训练模型</a></li>
</ol>

  </article>
  <!-- tag -->
  <div class="mt-12 pt-6 border-t border-gray-200">
    
      
        <span class="bg-gray-100 dark:bg-gray-700 px-2 py-1 m-1 text-sm rounded-md transition-colors hover:bg-gray-200">
          <a href="/tags/CV%E6%A8%A1%E5%9E%8B/">CV模型</a>
        </span>
      
        <span class="bg-gray-100 dark:bg-gray-700 px-2 py-1 m-1 text-sm rounded-md transition-colors hover:bg-gray-200">
          <a href="/tags/vision-transformer%E7%B3%BB%E5%88%97/">vision transformer系列</a>
        </span>
      
    
  </div>
  <!-- prev and next -->
  <div class="flex justify-between mt-12 pt-6 border-t border-gray-200">
    <div>
      
        <a href="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/prompt%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/" class="text-sm text-gray-400 hover:text-gray-500 flex justify-center">
          <iconify-icon width="20" icon="ri:arrow-left-s-line" data-inline="false"></iconify-icon>
          prompt基础知识
        </a>
      
    </div>
    <div>
      
        <a href="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CV%E6%96%B9%E5%90%91/vit%E7%B3%BB%E5%88%97/" class="text-sm text-gray-400 hover:text-gray-500 flex justify-center">
          ViT系列模型学习
          <iconify-icon width="20" icon="ri:arrow-right-s-line" data-inline="false"></iconify-icon>
        </a>
      
    </div>
  </div>
  <!-- comment -->
  <div class="article-comments mt-12">
    

  </div>
</section>
<!-- js inspect -->

<script src="/lib/clipboard.min.js"></script>


<script async src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
  });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>



<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>
  $(document).ready(() => {
    const maraidConfig = {
      theme: "default",
      logLevel: 3,
      flowchart: { curve: "linear" },
      gantt: { axisFormat: "%m/%d/%Y" },
      sequence: { actorMargin: 50 },
    };
    mermaid.initialize(maraidConfig);
  });
</script>



<script src="/lib/fancybox/fancybox.umd.min.js"></script>

<script>
  $(document).ready(() => {
    $('.post-content').each(function(i){
      $(this).find('img').each(function(){
        if ($(this).parent().hasClass('fancybox') || $(this).parent().is('a')) return;
        var alt = this.alt;
        if (alt) $(this).after('<span class="fancybox-alt">' + alt + '</span>');
        $(this).wrap('<a class="fancybox-img" href="' + this.src + '" data-fancybox=\"gallery\" data-caption="' + alt + '"></a>')
      });
      $(this).find('.fancybox').each(function(){
        $(this).attr('rel', 'article' + i);
      });
    });

    Fancybox.bind('[data-fancybox="gallery"]', {
        // options
    })
  })
</script>

<!-- tocbot begin -->

<script src="/lib/tocbot/tocbot.min.js"></script>

<script>
  $(document).ready(() => {
      tocbot.init({
        // Where to render the table of contents.
        tocSelector: '.post-toc',
        // Where to grab the headings to build the table of contents.
        contentSelector: '.post-content',
        // Which headings to grab inside of the contentSelector element.
        headingSelector: 'h1, h2, h3',
        // For headings inside relative or absolute positioned containers within content.
        hasInnerContainers: true,
    });
  })
</script>
<!-- tocbot end -->


  </main>
  <footer class="flex flex-col h-40 items-center justify-center text-gray-400 text-sm">
  <!-- busuanzi -->
  
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<!-- Busuanzi Analytics -->
<div class="flex items-center gap-2">
  <span>Visitors</span>
  <span id="busuanzi_value_site_uv"></span>
  <span>Page Views</span>
  <span id="busuanzi_value_site_pv"></span>
</div>
<!-- End Busuanzi Analytics -->


  <!-- copyright -->
  <div class="flex items-center gap-2">
    <a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" style="color: inherit;">CC BY-NC-SA 4.0</a>
    <span>© 2022</span>
    <iconify-icon width="18" icon="emojione-monotone:maple-leaf" ></iconify-icon>
    <a href="https://github.com/xbmlz" target="_blank" rel="noopener noreferrer">xbmlz</a>
  </div>
  <!-- powered by -->
  <div class="flex items-center gap-2">
    <span>Powered by</span>
    <a href="https://hexo.io/" target="_blank" rel="noopener noreferrer">Hexo</a>
    <span>&</span>
    <a href="https://github.com/xbmlz/hexo-theme-maple" target="_blank" rel="noopener noreferrer">Maple</a>
  </div>

</footer>

  <div class="back-to-top box-border fixed right-6 z-1024 -bottom-20 rounded py-1 px-1 bg-slate-900 opacity-60 text-white cursor-pointer text-center dark:bg-slate-600">
    <span class="flex justify-center items-center text-sm">
      <iconify-icon width="18" icon="ion:arrow-up-c" id="go-top"></iconify-icon>
      <span id="scrollpercent"><span>0</span> %</span>
    </span>
  </div>
  
<script src="/js/main.js"></script>


  <script>
    $(document).ready(function () {
      const mapleCount = "10";
      const speed = "0.5";
      const mapleEl = document.getElementById("maple");
      const maples = Array.from({ length: mapleCount }).map(() => {
        const maple = document.createElement("div");
        const scale = Math.random() * 0.5 + 0.5;
        const offset = Math.random() * 2 - 1;
        const x = Math.random() * mapleEl.clientWidth;
        const y = -Math.random() * mapleEl.clientHeight;
        const duration = 10 / speed;
        const delay = -duration;
        maple.className = "maple";
        maple.style.width = `${24 * scale}px`;
        maple.style.height = `${24 * scale}px`;
        maple.style.left = `${x}px`;
        maple.style.top = `${y}px`;
        maple.style.setProperty("--maple-fall-offset", offset);
        maple.style.setProperty("--maple-fall-height", `${Math.abs(y) + mapleEl.clientHeight}px`);
        maple.style.animation = `fall ${duration}s linear infinite`;
        maple.style.animationDelay = `${delay}s`;
        mapleEl.appendChild(maple)
        return maple
      })
    });
  </script>
  


  <div class="fixed top-0 bottom-0 left-0 right-0 pointer-events-none print:hidden" id="maple"></div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>
