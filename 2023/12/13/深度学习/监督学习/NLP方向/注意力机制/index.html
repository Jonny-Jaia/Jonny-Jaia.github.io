<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","Pisces | Gemini":240,"display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Transformer References   位置编码相关工作 总述 绝对位置编码 相对位置编码 参考   RoPE   不同激活函数与优化器设计 优化器 激活函数 SwiGLU 参考     模型结构微调 Norm Pre-Norm Norm类型 参考     不同attention实现 Flash-Attention PageAttention Multi-Query Attention">
<meta property="og:type" content="article">
<meta property="og:title" content="注意力机制笔记">
<meta property="og:url" content="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/index.html">
<meta property="og:site_name" content="JiangYh&#39;s Blog">
<meta property="og:description" content="Transformer References   位置编码相关工作 总述 绝对位置编码 相对位置编码 参考   RoPE   不同激活函数与优化器设计 优化器 激活函数 SwiGLU 参考     模型结构微调 Norm Pre-Norm Norm类型 参考     不同attention实现 Flash-Attention PageAttention Multi-Query Attention">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/640.png">
<meta property="og:image" content="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/v2-7792863eb59d96636cbddbf85788c1c4_1440w.webp">
<meta property="og:image" content="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image.png">
<meta property="og:image" content="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20230718230559320.png">
<meta property="og:image" content="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20230718230640108.png">
<meta property="og:image" content="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/v2-1520514ff5f2c6ea2a0def6ace10bb42_720w.png">
<meta property="og:image" content="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/v2-a7a7b4ad7ce1018183e1b8eb654f3f91_1440w.webp">
<meta property="og:image" content="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20230712101727279.png">
<meta property="og:image" content="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/v2-dcd81bd8ea2fcba1e777f700ac1e5146_1440w.webp">
<meta property="og:image" content="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/v2-f1eb53988e49ac7358d2af489ec4b9bd_1440w.webp">
<meta property="og:image" content="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/98a70456797347a68085cce705d71ed3.png">
<meta property="article:published_time" content="2023-12-13T14:56:46.394Z">
<meta property="article:modified_time" content="2023-12-13T14:58:36.718Z">
<meta property="article:author" content="JiangYH">
<meta property="article:tag" content="Transformer模型">
<meta property="article:tag" content="位置编码">
<meta property="article:tag" content="self-attention">
<meta property="article:tag" content="Norm位置结构">
<meta property="article:tag" content="Bert与GPT">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/640.png">


<link rel="canonical" href="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/","path":"2023/12/13/深度学习/监督学习/NLP方向/注意力机制/","title":"注意力机制笔记"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>注意力机制笔记 | JiangYh's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">JiangYh's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Transformer"><span class="nav-number">1.</span> <span class="nav-text">Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">1.1.</span> <span class="nav-text">References</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="nav-number">2.</span> <span class="nav-text">位置编码相关工作</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E8%BF%B0"><span class="nav-number">2.1.</span> <span class="nav-text">总述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%9D%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="nav-number">2.1.1.</span> <span class="nav-text">绝对位置编码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="nav-number">2.1.2.</span> <span class="nav-text">相对位置编码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">2.1.3.</span> <span class="nav-text">参考</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RoPE"><span class="nav-number">2.2.</span> <span class="nav-text">RoPE</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%8D%E5%90%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E5%99%A8%E8%AE%BE%E8%AE%A1"><span class="nav-number">3.</span> <span class="nav-text">不同激活函数与优化器设计</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="nav-number">3.1.</span> <span class="nav-text">优化器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">3.2.</span> <span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SwiGLU"><span class="nav-number">3.2.1.</span> <span class="nav-text">SwiGLU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83-1"><span class="nav-number">3.2.2.</span> <span class="nav-text">参考</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E5%BE%AE%E8%B0%83"><span class="nav-number">4.</span> <span class="nav-text">模型结构微调</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Norm"><span class="nav-number">4.1.</span> <span class="nav-text">Norm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Pre-Norm"><span class="nav-number">4.1.1.</span> <span class="nav-text">Pre-Norm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Norm%E7%B1%BB%E5%9E%8B"><span class="nav-number">4.1.2.</span> <span class="nav-text">Norm类型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83-2"><span class="nav-number">4.1.3.</span> <span class="nav-text">参考</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%8D%E5%90%8Cattention%E5%AE%9E%E7%8E%B0"><span class="nav-number">5.</span> <span class="nav-text">不同attention实现</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Flash-Attention"><span class="nav-number">5.1.</span> <span class="nav-text">Flash-Attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PageAttention"><span class="nav-number">5.2.</span> <span class="nav-text">PageAttention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multi-Query-Attention"><span class="nav-number">5.3.</span> <span class="nav-text">Multi-Query Attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Group-Query-Attention"><span class="nav-number">5.4.</span> <span class="nav-text">Group-Query Attention</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Bert%E4%B8%8EGPT%E7%9A%84%E4%B8%8D%E5%90%8C"><span class="nav-number">6.</span> <span class="nav-text">Bert与GPT的不同</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="nav-number">6.1.</span> <span class="nav-text">自回归模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#GPT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B"><span class="nav-number">6.1.1.</span> <span class="nav-text">GPT系列模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E7%BC%96%E7%A0%81%E6%A8%A1%E5%9E%8B"><span class="nav-number">6.2.</span> <span class="nav-text">自编码模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Encoder-Decoder"><span class="nav-number">6.3.</span> <span class="nav-text">Encoder-Decoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83-3"><span class="nav-number">6.4.</span> <span class="nav-text">参考</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">JiangYH</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">35</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">77</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/yourname" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yourname" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="JiangYH">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="注意力机制笔记 | JiangYh's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          注意力机制笔记
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-12-13 22:56:46 / Modified: 22:58:36" itemprop="dateCreated datePublished" datetime="2023-12-13T22:56:46+08:00">2023-12-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><ul>
<li><a href="#transformer">Transformer</a><ul>
<li><a href="#references">References</a></li>
</ul>
</li>
<li><a href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C">位置编码相关工作</a><ul>
<li><a href="#%E6%80%BB%E8%BF%B0">总述</a><ul>
<li><a href="#%E7%BB%9D%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81">绝对位置编码</a></li>
<li><a href="#%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81">相对位置编码</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
</li>
<li><a href="#rope">RoPE</a></li>
</ul>
</li>
<li><a href="#%E4%B8%8D%E5%90%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E5%99%A8%E8%AE%BE%E8%AE%A1">不同激活函数与优化器设计</a><ul>
<li><a href="#%E4%BC%98%E5%8C%96%E5%99%A8">优化器</a></li>
<li><a href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">激活函数</a><ul>
<li><a href="#swiglu">SwiGLU</a></li>
<li><a href="#%E5%8F%82%E8%80%83-1">参考</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E5%BE%AE%E8%B0%83">模型结构微调</a><ul>
<li><a href="#norm">Norm</a><ul>
<li><a href="#pre-norm">Pre-Norm</a></li>
<li><a href="#norm%E7%B1%BB%E5%9E%8B">Norm类型</a></li>
<li><a href="#%E5%8F%82%E8%80%83-2">参考</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E4%B8%8D%E5%90%8Cattention%E5%AE%9E%E7%8E%B0">不同attention实现</a><ul>
<li><a href="#flash-attention">Flash-Attention</a></li>
<li><a href="#pageattention">PageAttention</a></li>
<li><a href="#multi-query-attention">Multi-Query Attention</a></li>
<li><a href="#group-query-attention">Group-Query Attention</a></li>
</ul>
</li>
<li><a href="#bert%E4%B8%8Egpt%E7%9A%84%E4%B8%8D%E5%90%8C">Bert与GPT的不同</a><ul>
<li><a href="#%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B">自回归模型</a><ul>
<li><a href="#gpt%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B">GPT系列模型</a></li>
</ul>
</li>
<li><a href="#%E8%87%AA%E7%BC%96%E7%A0%81%E6%A8%A1%E5%9E%8B">自编码模型</a></li>
<li><a href="#encoder-decoder">Encoder-Decoder</a></li>
<li><a href="#%E5%8F%82%E8%80%83-3">参考</a></li>
</ul>
</li>
</ul>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>结构篇：</p>
<p><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/640.png" alt="transformer结构"></p>
<p>encoder-decoder:6 block</p>
<p>相关问题</p>
<ol>
<li><p>self-attention实现</p>
<p> $Softmax\frac{(Q*k)}{\sqrt{d_k}}V$，其中$d_k$是Q,K的列数，防止内积过大；可以使得输入的数据的分布变得更好，防止梯度消失，让模型能够更容易训练。</p>
<p> 只要能够建模相关性，别的建模方式也能够代替当前的自注意力计算；同样的能够缓解梯度消失问题也不用除掉列数值。</p>
<p> 对位置信息不敏感，需要增加pos-emb；embedding 的直接相加,类似于信号的叠加，只要保证频率不同叠加的信号就能够再后续发挥作用。</p>
<p> QKV的不同主要是为了增强容量和表达能力。多头也是为了增加参数量进而增强模型的表达能力，</p>
</li>
<li><p>整体的维度变化</p>
<p> input:(bs,max_len)<br> embedding:(bs,max_len,hidden_size)<br> MHA:(bs,max_len,hidden_size)</p>
<pre><code> Q(K,V): (bs,max_len,hidden_size)
 多头机制：
 input:(bs*num_heads,max_len,hidden_size//num_heads)
 output:(bs*num_heads,max_len,hidden_size//num_heads)
 concat&amp;Linear:(bs,max_len,hidden_size)
</code></pre>
<p> add&amp;Post-Norm:(bs,max_len,hidden_size)<br> FF:(bs,max_len,hidden_size)</p>
<pre><code> 先升维再降维
 FF1:(bs,max_len,hidden_size*4)
 FF2:(bs,max_len,hidden_size)
</code></pre>
</li>
<li><p>计算复杂度对比</p>
</li>
</ol>
<p><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/v2-7792863eb59d96636cbddbf85788c1c4_1440w.webp" alt="img"></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzAxMTU5Njg4NQ==&mid=2247490647&idx=1&sn=a711ed1c556cdae1821bab251ee3893f&chksm=9bbff96dacc8707bdb133acfa55d599ca8dcea47027aa1fc2344d06d84584dbd4981c939fee0&scene=21#wechat_redirect">【关于Transformer】 那些的你不知道的事（上）</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/neumy/p/15932615.html">从Attention 到 MultiHeadAttention</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/338817680">Transformer模型详解（图解最完整版）</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/hyzhyzhyz12345/article/details/104119375">说说transformer当中的维度变化</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/643560888">大模型面试八股</a></li>
</ol>
<h1 id="位置编码相关工作"><a href="#位置编码相关工作" class="headerlink" title="位置编码相关工作"></a>位置编码相关工作</h1><h2 id="总述"><a href="#总述" class="headerlink" title="总述"></a>总述</h2><p>基于自注意力机制本身的计算原理，其对位置信息不敏感，具体从公式角度来看：</p>
<p><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image.png" alt="Attention-cal"></p>
<p>调换序列中两个元素的位置不会影响到当前的注意力得分计算。</p>
<h3 id="绝对位置编码"><a href="#绝对位置编码" class="headerlink" title="绝对位置编码"></a>绝对位置编码</h3><ul>
<li>训练式</li>
</ul>
<p>直接将位置编码当作可训练参数。一般的认为，该方法的缺点在于没有外推性，超过预设窗口大小的内容就无法处理了。(当前有一些可以通过如层次分解的方法将位置编码外推足够长的范围)</p>
<ul>
<li>三角式-transformer：</li>
</ul>
<p><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20230718230559320.png" alt="image-20230718230559320"></p>
<p><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20230718230640108.png" alt="image-20230718230640108"></p>
<p>通过内积的方法将相对位置信息融入到特征中。但具体实现中，由于参数矩阵也需要参与计算$p^{T}<em>{t}W^{T}</em>{Q}W_{K}p_s \not &#x3D;{p^{T}_{t}p_s}.$经相关研究，可知由于参数矩阵使得余弦波不再是理想情况，无法真正感知元素的相对未知信息。</p>
<ul>
<li>递归式</li>
</ul>
<p>本质思想是RNN这类递归模型，学习位置编码，再接入transformer。但最大的问题是牺牲了一定的并行性，会带来速度瓶颈。</p>
<h3 id="相对位置编码"><a href="#相对位置编码" class="headerlink" title="相对位置编码"></a>相对位置编码</h3><p>相对位置编码在计算自注意力矩阵时，根据矩阵元素的下标，直接考虑每个元素对应的两个token间的相对位置关系。此外，相比于绝对位置编码仅仅在输入层考虑顺序特征，相对位置编码则通过修改自注意力计算的过程，植入到Transformer架构的每一层。</p>
<p>不同模型的相对位置编码：</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/631363482">Transformer位置编码（基础）</a></li>
<li><a target="_blank" rel="noopener" href="https://kexue.fm/archives/8130">让研究人员绞尽脑汁的Transformer位置编码</a></li>
<li></li>
</ol>
<h2 id="RoPE"><a href="#RoPE" class="headerlink" title="RoPE"></a><a target="_blank" rel="noopener" href="https://gitee.com/Jonny-Jaia/ready-blog/blob/master/%E6%9D%82/4-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E4%B9%A0/LongContextWindow.md#rope">RoPE</a></h2><h1 id="不同激活函数与优化器设计"><a href="#不同激活函数与优化器设计" class="headerlink" title="不同激活函数与优化器设计"></a>不同激活函数与优化器设计</h1><h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p>AdamW</p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="SwiGLU"><a href="#SwiGLU" class="headerlink" title="SwiGLU"></a>SwiGLU</h3><ol>
<li>siwish-线性函数与ReLU之间的平滑</li>
</ol>
<p>$f(x)&#x3D;x \times sigmoid(\beta x)$</p>
<ol start="2">
<li>GELU-高斯误差线性单元，RELU的变种</li>
</ol>
<p>$f(x)&#x3D;x \times \phi(x),\phi(x)是正态分布的累积函数$，和Swish形式性质相似，表现相当</p>
<ol start="3">
<li>GLU-门控<br>$\text{GLU}(a, b) &#x3D; a \otimes \sigma(b)$<br><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/v2-1520514ff5f2c6ea2a0def6ace10bb42_720w.png"></li>
</ol>
<p>具体就是首先通过中间向量g(x)&#x3D;xW进行门控操作，使用Sigmoid函数σ将其映射到0到1之间的范围，表示每个元素被保留的概率。然后，将输入向量x与门控后的向量进行逐元素相乘（即 ⊗ 操作），得到最终的输出向量。</p>
<ol start="4">
<li>GEGL-GLU变体</li>
</ol>
<p>就是将GLU中的sigmoid激活函数替换成GELU激活函数<br>5. SwiGLU<br><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/v2-a7a7b4ad7ce1018183e1b8eb654f3f91_1440w.webp" alt="swishglu"></p>
<p>就是将GLU中的sigmoid激活函数替换成Swish激活函数</p>
<h3 id="参考-1"><a href="#参考-1" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/638735903">大语言模型综述&lt;演进，技术路线，区别，微调，实践，潜在问题与讨论&gt;</a></li>
</ol>
<h1 id="模型结构微调"><a href="#模型结构微调" class="headerlink" title="模型结构微调"></a>模型结构微调</h1><h2 id="Norm"><a href="#Norm" class="headerlink" title="Norm"></a>Norm</h2><h3 id="Pre-Norm"><a href="#Pre-Norm" class="headerlink" title="Pre-Norm"></a>Pre-Norm</h3><ul>
<li>LayerNorm会影响训练的稳定性</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://readpaper.com/pdf-annotate/note?pdfId=4500336028413485057&noteId=1867628851738139648">Megatron-LM</a> 用实验证明layernorm后置效果要更加稳定</p>
</blockquote>
<p><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20230712101727279.png" alt="image-20230712101727279"></p>
<p>Post-LN（原始的BERT）</p>
<p>Pre-LN：On layer normalization in the transformer architecture</p>
<p>Sandwich-LN: Cogview: Mastering text-to-image generation via transformers</p>
<p>通常认为稳定性上: Sandwich-LN &gt; Pre-LN &gt; Post-LN</p>
<h3 id="Norm类型"><a href="#Norm类型" class="headerlink" title="Norm类型"></a>Norm类型</h3><ul>
<li><p>LayerNorm<br>传统transformer-Post-LN、随着层数加深梯度范数会增大导致训练不稳定。<br>Pre-LN:使用pre LN的深层transformer训练更稳定，可以缓解训练不稳定问题。但缺点是pre LN可能会轻微影响transformer模型的性能 大语言模型的一个挑战就是如何提升训练的稳定性。<br><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/v2-dcd81bd8ea2fcba1e777f700ac1e5146_1440w.webp" alt="LayerNorm"></p>
<p>Norm中采用的性质：</p>
<ol>
<li>平移不变性：均值</li>
<li>缩放不变性：方差</li>
</ol>
</li>
<li><p>RMSNorm<br>只保留缩放，简化计算的同时，效果基本相当甚至还略有提升。<br><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/v2-f1eb53988e49ac7358d2af489ec4b9bd_1440w.webp" alt="RMSNorm"></p>
</li>
<li><p>DeepNorm</p>
<p><img src="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/98a70456797347a68085cce705d71ed3.png" alt="deepnorm"></p>
<p>用以缓解爆炸式模型更新的问题，更可以再此基础上实现千层堆积。<br>$x &#x3D; LayerNorm(x \times \alpha + f(x))$</p>
<ol>
<li>DeepNorm在进行Layer Norm之前会以 α参数扩大残差连接</li>
<li>在Xavier参数初始化过程中以 β减小部分参数的初始化范围</li>
</ol>
</li>
</ul>
<h3 id="参考-2"><a href="#参考-2" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/638735903">大语言模型综述&lt;演进，技术路线，区别，微调，实践，潜在问题与讨论&gt;</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/479860623">DEEPNORM：千层transformer…</a></li>
</ol>
<h1 id="不同attention实现"><a href="#不同attention实现" class="headerlink" title="不同attention实现"></a>不同attention实现</h1><h2 id="Flash-Attention"><a href="#Flash-Attention" class="headerlink" title="Flash-Attention"></a><a target="_blank" rel="noopener" href="https://gitee.com/Jonny-Jaia/ready-blog/blob/master/%E6%9D%82/4-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E4%B9%A0/FlashAttention.md#flashattention">Flash-Attention</a></h2><h2 id="PageAttention"><a href="#PageAttention" class="headerlink" title="PageAttention"></a><a target="_blank" rel="noopener" href="https://gitee.com/Jonny-Jaia/ready-blog/blob/master/%E6%9D%82/4-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E4%B9%A0/LLM%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F.md#vllmpagedattention">PageAttention</a></h2><h2 id="Multi-Query-Attention"><a href="#Multi-Query-Attention" class="headerlink" title="Multi-Query Attention"></a><a href>Multi-Query Attention</a></h2><h2 id="Group-Query-Attention"><a href="#Group-Query-Attention" class="headerlink" title="Group-Query Attention"></a><a href>Group-Query Attention</a></h2><h1 id="Bert与GPT的不同"><a href="#Bert与GPT的不同" class="headerlink" title="Bert与GPT的不同"></a>Bert与GPT的不同</h1><h2 id="自回归模型"><a href="#自回归模型" class="headerlink" title="自回归模型"></a>自回归模型</h2><p>AR，代表GPT，从左向右学习。</p>
<p>AR模型通常用于生成式任务，在长文本的生成能力很强，比如自然语言生成（NLG）领域的任务：摘要、翻译或抽象问答。</p>
<p>具体来说，就是利用上文词预测下一个词的发生概率。</p>
<p>优点：AR模型擅长生成式NLP任务。AR模型使用注意力机制，预测下一个token，因此自然适用于文本生成。此外，AR模型可以简单地将训练目标设置为预测语料库中的下一个token，因此生成数据相对容易。</p>
<p>缺点：AR模型只能用于前向或者后向建模，不能同时使用双向的上下文信息，不能完全捕捉token的内在联系。</p>
<h3 id="GPT系列模型"><a href="#GPT系列模型" class="headerlink" title="GPT系列模型"></a>GPT系列模型</h3><ul>
<li><p>GPT1<br>通过无监督预训练+有监督微调实现模型性能的提升。另外，将预训练目标作为辅助目标加入下游任务loss中，将会提高有监督模型的泛化性能，并加速收敛。</p>
</li>
<li><p>GPT2-15B</p>
</li>
</ul>
<blockquote>
<p> “所有的有监督学习都是无监督语言模型的一个子集”</p>
</blockquote>
<p>  增大了模型大小与参数规模，提出了zero-shot，并且提出了以一个通用预训练模型为基础，使得下游任务无需手动生成或标记训练数据集，更不需要修改预训练模型的参数或结构。</p>
<p>  GPT2通过实验验证了海量数据与大量参数训练得到的语言模型可以迁移到下游其他任务中，无需额外训练和微调。</p>
<ul>
<li><p>GPT3-175B<br>引入了In-Context Learning的概念，GPT3参数量增大的同时，期望不通过微调直接能够通过上下文指示也能够有较好的性能。</p>
<p>In-Context learning是元学习（Meta-learning）的一种，元学习的核心思想在于通过<strong>少量的数据寻找一个合适的初始化范围</strong>，使得模型能够在有限的数据集上快速拟合，并获得不错的效果。</p>
</li>
<li><p>InstructGPT</p>
<blockquote>
<p>提出动机：让模型的输出达到3H(helpful,honest,harmless)</p>
</blockquote>
<ol>
<li>RLHF</li>
</ol>
<p>  人类喜欢的内容大致符合以上的3H标准，并且也能够保证生成内容流畅性与语法正确性；</p>
<p>  通过RL指导模型训练，以人类反馈作为奖励，实现将人类经验内容的注入。</p>
<ol start="2">
<li>实验步骤</li>
</ol>
<p>  有监督微调-基于人工标注的对比数据训练奖励模型-基于RM利用PPO微调SFT模型；</p>
<p>  三部分数据集：<br>  SFT数据：简单任务、few-shot任务、用户相关的任务；<br>  RM数据：让模型先生成一批候选文本，然后针对这部分数据进行排序；<br>  PPO数据：无标注数据，来自GPT3的API用户调用任务数据；</p>
<p>  训练设置：<br>  SFT：与GPT3一致，适当过拟合有助于后续的训练；<br>  RM：输入prompt和response，输出奖励值；训练过程中将同一个prompt的k个输出成对取出共有$C_{K}^{2}$个结果作为一个batch输入，loss就是最大化结果的差值；<br>  PPO：KL惩罚确保两个策略的输出差距不会很大；为了防止模型在通用NLP任务上性能大幅度下降，优化目标中增加了通用语言模型的目标；</p>
<ol start="3">
<li>优缺点</li>
</ol>
<p>  优点：结果更真实，无害性提高，coding能力提升；<br>  缺点：会降低在通用NLP任务上的效果；依然会给出奇怪的输出；对指示十分敏感；对简单概念过分解读。</p>
</li>
<li><p>GPT4-1.8T</p>
</li>
</ul>
<ol>
<li>模型架构</li>
</ol>
<p>采用的是MoE的架构</p>
<ol start="2">
<li>数据组成</li>
</ol>
<h2 id="自编码模型"><a href="#自编码模型" class="headerlink" title="自编码模型"></a>自编码模型</h2><p>AE，代表BERT，主要是对掩码部分能够实现重建，常用于内容理解任务，比如自然语言理解（NLU）中的分类任务：情感分析、提取式问答。</p>
<p>优点：在上下文依赖中，BERT的表示可以涵盖前后向两边的上下文。BERT使用双向transformer，在语言理解相关的任务中表现很好。</p>
<p>缺点：</p>
<ul>
<li>输入噪声：<br>BERT在预训练过程中使用【mask】符号对输入进行处理，这些符号在下游的finetune任务中永远不会出现，这会导致预训练-微调差异。而AR模型不会依赖于任何被mask的输入，因此不会遇到这类问题。</li>
<li>BERT在对联合条件概率进行因式分解时，基于一个独立假设：在给定了unmasked tokens时，所有待预测（masked）的tokens是相互独立的。</li>
</ul>
<h2 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h2><p>同时包含编码器和解码器两部分，常用的有T5、BART等模型</p>
<h2 id="参考-3"><a href="#参考-3" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/626494749">[万字长文]ChatGPT系列论文精读——大模型经典论文GPT1、GPT2、GPT3</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/625714067">一文读懂GPT家族和BERT的底层区别——自回归和自编码语言模型详解</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/642927542">大规模语言模型（LLMs）预训练十六: GPT-4大揭密</a></li>
<li><a target="_blank" rel="noopener" href="https://www.semianalysis.com/p/gpt-4-architecture-infrastructure">GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE (semianalysis.com)</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/590311003">ChatGPT&#x2F;InstructGPT详解</a></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Transformer%E6%A8%A1%E5%9E%8B/" rel="tag"># Transformer模型</a>
              <a href="/tags/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/" rel="tag"># 位置编码</a>
              <a href="/tags/self-attention/" rel="tag"># self-attention</a>
              <a href="/tags/Norm%E4%BD%8D%E7%BD%AE%E7%BB%93%E6%9E%84/" rel="tag"># Norm位置结构</a>
              <a href="/tags/Bert%E4%B8%8EGPT/" rel="tag"># Bert与GPT</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/" rel="prev" title="条件随机场">
                  <i class="fa fa-angle-left"></i> 条件随机场
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%96%B9%E5%90%91/LLMs/" rel="next" title="大模型相关技术">
                  大模型相关技术 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">JiangYH</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
