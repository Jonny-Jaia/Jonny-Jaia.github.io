<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">




<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

  <meta name="author" content="JiangYH">





<title>注意力机制笔记 | 通往成功的道路</title>



<link rel="icon" href="/favicon.png">



<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/nprogress/nprogress.css">



<script src="/lib/jquery.min.js"></script>


<script src="/lib/iconify-icon.min.js"></script>


<script src="https://cdn.tailwindcss.com?plugins=typography"></script>
<script>
  tailwind.config = {
    darkMode: "class",
  };
</script>


<script src="/lib/nprogress/nprogress.js"></script>

<script>
  $(document).ready(() => {
    NProgress.configure({
      showSpinner: false,
    });
    NProgress.start();
    $("#nprogress .bar").css({
      background: "#de7441",
    });
    $("#nprogress .peg").css({
      "box-shadow": "0 0 2px #de7441, 0 0 4px #de7441",
    });
    $("#nprogress .spinner-icon").css({
      "border-top-color": "#de7441",
      "border-left-color": "#de7441",
    });
    setTimeout(function () {
      NProgress.done();
      $(".fade").removeClass("out");
    }, 800);
  });
</script>

<script>
  (function () {
    const prefersDark =
      window.matchMedia &&
      window.matchMedia("(prefers-color-scheme: dark)").matches;
    const setting = localStorage.getItem("hexo-color-scheme") || "auto";
    if (setting === "dark" || (prefersDark && setting !== "light"))
      document.documentElement.classList.toggle("dark", true);
    let isDark = document.documentElement.classList.contains("dark");
  })();

  $(document).ready(function () {
    // init icon
    const prefersDark =
      window.matchMedia &&
      window.matchMedia("(prefers-color-scheme: dark)").matches;
    const isDark = document.documentElement.classList.contains("dark");
    $("#theme-icon").attr("icon", isDark ? "ri:moon-line" : "ri:sun-line");

    function toggleGiscusTheme() {
      const isDark = document.documentElement.classList.contains("dark");
      const giscusFrame = document.querySelector("iframe.giscus-frame");
      if (giscusFrame) {
        giscusFrame.contentWindow.postMessage(
          {
            giscus: {
              setConfig: {
                theme: isDark ? "dark" : "light",
              },
            },
          },
          "https://giscus.app"
        );
      }
    }


    // toggle dark mode
    function toggleDark() {
      let isDark = document.documentElement.classList.contains("dark");
      const setting = localStorage.getItem("hexo-color-scheme") || "auto";
      isDark = !isDark;
      document.documentElement.classList.toggle("dark", isDark);
      $("#theme-icon").attr("icon", isDark ? "ri:moon-line" : "ri:sun-line");
      if (prefersDark === isDark) {
        localStorage.setItem("hexo-color-scheme", "auto");
      } else {
        localStorage.setItem("hexo-color-scheme", isDark ? "dark" : "light");
      }
      toggleGiscusTheme();
    }
    $("#toggle-dark").click(toggleDark);

    // listen dark mode change
    window
      .matchMedia("(prefers-color-scheme: dark)")
      .addEventListener("change", (e) => {
        const setting = localStorage.getItem("hexo-color-scheme") || "auto";
        if (setting === "auto") {
          document.documentElement.classList.toggle("dark", e.matches);
          $("#theme-icon").attr(
            "icon",
            e.matches ? "ri:moon-line" : "ri:sun-line"
          );
          toggleGiscusTheme();
        }
      });
  });
</script>




<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head>
<body class="font-sans bg-white dark:bg-zinc-900 text-gray-700 dark:text-gray-200 relative">
  <header class="fixed w-full px-5 py-1 z-10 backdrop-blur-xl backdrop-saturate-150 border-b border-black/5">
  <div class="max-auto">
    <nav class="flex items-center text-base">
      <a href="/" class="group">
        <h2 class="font-medium tracking-tighterp text-l p-2">
          <img class="w-5 mr-2 inline-block transition-transform group-hover:rotate-[30deg]" id="logo" src="/images/logo.svg" alt="通往成功的道路" />
          通往成功的道路
        </h2>
      </a>
      <div id="header-title" class="opacity-0 md:ml-2 md:mt-[0.1rem] text-xs font-medium whitespace-nowrap overflow-hidden overflow-ellipsis">
        注意力机制笔记
      </div>
      <div class="flex-1"></div>
      <div class="flex items-center gap-3">
        
          <a class="hidden sm:flex" href="/archives">Posts</a>
        
          <a class="hidden sm:flex" href="/category">Categories</a>
        
          <a class="hidden sm:flex" href="/tag">Tags</a>
        
        
          
            <a class="w-5 h-5 hidden sm:flex" title="Github" target="_blank" rel="noopener" href="https://github.com/xbmlz">
              <iconify-icon width="20" icon="ri:github-line"></iconify-icon>
            </a>
          
        
        <a class="w-5 h-5 hidden sm:flex" title="Github" href="rss2.xml">
          <iconify-icon width="20" icon="ri:rss-line"></iconify-icon>
        </a>
        <a class="w-5 h-5" title="toggle theme" id="toggle-dark">
          <iconify-icon width="20" icon="" id="theme-icon"></iconify-icon>
        </a>
      </div>
      <div class="flex items-center justify-center gap-3 ml-3 sm:hidden">
        <span class="w-5 h-5" aria-hidden="true" role="img" id="open-menu">
          <iconify-icon width="20" icon="carbon:menu" ></iconify-icon>
        </span>
        <span class="w-5 h-5 hidden" aria-hidden="true" role="img" id="close-menu">
          <iconify-icon  width="20" icon="carbon:close" ></iconify-icon>
        </span>
      </div>
    </nav>
  </div>
</header>
<div id="menu-panel" class="h-0 overflow-hidden sm:hidden fixed left-0 right-0 top-12 bottom-0 z-10">
  <div id="menu-content" class="relative z-20 bg-white/80 px-6 sm:px-8 py-2 backdrop-blur-xl -translate-y-full transition-transform duration-300">
    <ul class="nav flex flex-col sm:flex-row text-sm font-medium">
      
        <li class="nav-portfolio sm:mx-2 border-b sm:border-0 border-black/5 last:border-0 hover:text-main">
          <a href="/archives" class="flex h-12 sm:h-auto items-center">Posts</a>
        </li>
      
        <li class="nav-portfolio sm:mx-2 border-b sm:border-0 border-black/5 last:border-0 hover:text-main">
          <a href="/category" class="flex h-12 sm:h-auto items-center">Categories</a>
        </li>
      
        <li class="nav-portfolio sm:mx-2 border-b sm:border-0 border-black/5 last:border-0 hover:text-main">
          <a href="/tag" class="flex h-12 sm:h-auto items-center">Tags</a>
        </li>
      
    </ul>
  </div>
  <div class="mask bg-black/20 absolute inset-0"></div>
</div>

  <main class="pt-14">
    <!-- css -->

<link rel="stylesheet" href="/lib/fancybox/fancybox.min.css">


<link rel="stylesheet" href="/lib/tocbot/tocbot.min.css">

<!-- toc -->

  <!-- tocbot -->
<nav class="post-toc toc text-sm w-48 relative top-32 right-0 opacity-70 hidden lg:block" style="position: fixed !important;"></nav>


<section class="px-6 max-w-prose mx-auto md:px-0">
  <!-- header -->
  <header class="overflow-hidden pt-6 pb-6 md:pt-12">
    <div class="pt-4 md:pt-6">
      <h1 id="article-title" class="text-[2rem] font-bold leading-snug mb-4 md:mb-6 md:text-[2.6rem]">
        注意力机制笔记
      </h1>
      <div>
        <section class="flex items-center gap-3 text-sm">
          <span class="flex items-center gap-1">
            <iconify-icon width="18" icon="carbon-calendar" ></iconify-icon>
            <time>2023-12-13</time>
          </span>
          <span class="text-gray-400">·</span>
          <span class="flex items-center gap-1">
            <iconify-icon width="18" icon="ic:round-access-alarm" ></iconify-icon>
            <span>10 min</span>
          </span>
          <span class="text-gray-400">·</span>
          <span class="flex items-center gap-1">
            <iconify-icon width="18" icon="icon-park-outline:font-search" ></iconify-icon>
            <span>2.7k words</span>
          </span>
          
            <span class="text-gray-400">·</span>
            <span class="flex items-center gap-1">
              <iconify-icon width="16" icon="icon-park-outline:box" class="mr-2"></iconify-icon>
              <a class="article-category-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a>
            </span>
          
        </section>
      </div>
    </div>
  </header>
  <!-- content -->
  <article class="post-content prose m-auto slide-enter-content dark:prose-invert">
    <ul>
<li><a href="#transformer">Transformer</a><ul>
<li><a href="#references">References</a></li>
</ul>
</li>
<li><a href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C">位置编码相关工作</a><ul>
<li><a href="#%E6%80%BB%E8%BF%B0">总述</a><ul>
<li><a href="#%E7%BB%9D%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81">绝对位置编码</a></li>
<li><a href="#%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81">相对位置编码</a></li>
<li><a href="#%E5%8F%82%E8%80%83">参考</a></li>
</ul>
</li>
<li><a href="#rope">RoPE</a></li>
</ul>
</li>
<li><a href="#%E4%B8%8D%E5%90%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E5%99%A8%E8%AE%BE%E8%AE%A1">不同激活函数与优化器设计</a><ul>
<li><a href="#%E4%BC%98%E5%8C%96%E5%99%A8">优化器</a></li>
<li><a href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">激活函数</a><ul>
<li><a href="#swiglu">SwiGLU</a></li>
<li><a href="#%E5%8F%82%E8%80%83-1">参考</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E5%BE%AE%E8%B0%83">模型结构微调</a><ul>
<li><a href="#norm">Norm</a><ul>
<li><a href="#pre-norm">Pre-Norm</a></li>
<li><a href="#norm%E7%B1%BB%E5%9E%8B">Norm类型</a></li>
<li><a href="#%E5%8F%82%E8%80%83-2">参考</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E4%B8%8D%E5%90%8Cattention%E5%AE%9E%E7%8E%B0">不同attention实现</a><ul>
<li><a href="#flash-attention">Flash-Attention</a></li>
<li><a href="#pageattention">PageAttention</a></li>
<li><a href="#multi-query-attention">Multi-Query Attention</a></li>
<li><a href="#group-query-attention">Group-Query Attention</a></li>
</ul>
</li>
<li><a href="#bert%E4%B8%8Egpt%E7%9A%84%E4%B8%8D%E5%90%8C">Bert与GPT的不同</a><ul>
<li><a href="#%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B">自回归模型</a><ul>
<li><a href="#gpt%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B">GPT系列模型</a></li>
</ul>
</li>
<li><a href="#%E8%87%AA%E7%BC%96%E7%A0%81%E6%A8%A1%E5%9E%8B">自编码模型</a></li>
<li><a href="#encoder-decoder">Encoder-Decoder</a></li>
<li><a href="#%E5%8F%82%E8%80%83-3">参考</a></li>
</ul>
</li>
</ul>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>结构篇：</p>
<p><img src="/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.assets/640.png" alt="transformer结构"></p>
<p>encoder-decoder:6 block</p>
<p>相关问题</p>
<ol>
<li><p>self-attention实现</p>
<p> $Softmax\frac{(Q*k)}{\sqrt{d_k}}V$，其中$d_k$是Q,K的列数，防止内积过大；可以使得输入的数据的分布变得更好，防止梯度消失，让模型能够更容易训练。</p>
<p> 只要能够建模相关性，别的建模方式也能够代替当前的自注意力计算；同样的能够缓解梯度消失问题也不用除掉列数值。</p>
<p> 对位置信息不敏感，需要增加pos-emb；embedding 的直接相加,类似于信号的叠加，只要保证频率不同叠加的信号就能够再后续发挥作用。</p>
<p> QKV的不同主要是为了增强容量和表达能力。多头也是为了增加参数量进而增强模型的表达能力，</p>
</li>
<li><p>整体的维度变化</p>
<p> input:(bs,max_len)<br> embedding:(bs,max_len,hidden_size)<br> MHA:(bs,max_len,hidden_size)</p>
<pre><code> Q(K,V): (bs,max_len,hidden_size)
 多头机制：
 input:(bs*num_heads,max_len,hidden_size//num_heads)
 output:(bs*num_heads,max_len,hidden_size//num_heads)
 concat&amp;Linear:(bs,max_len,hidden_size)
</code></pre>
<p> add&amp;Post-Norm:(bs,max_len,hidden_size)<br> FF:(bs,max_len,hidden_size)</p>
<pre><code> 先升维再降维
 FF1:(bs,max_len,hidden_size*4)
 FF2:(bs,max_len,hidden_size)
</code></pre>
</li>
<li><p>计算复杂度对比</p>
</li>
</ol>
<p><img src="/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.assets/v2-7792863eb59d96636cbddbf85788c1c4_1440w.webp" alt="img"></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzAxMTU5Njg4NQ==&mid=2247490647&idx=1&sn=a711ed1c556cdae1821bab251ee3893f&chksm=9bbff96dacc8707bdb133acfa55d599ca8dcea47027aa1fc2344d06d84584dbd4981c939fee0&scene=21#wechat_redirect">【关于Transformer】 那些的你不知道的事（上）</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/neumy/p/15932615.html">从Attention 到 MultiHeadAttention</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/338817680">Transformer模型详解（图解最完整版）</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/hyzhyzhyz12345/article/details/104119375">说说transformer当中的维度变化</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/643560888">大模型面试八股</a></li>
</ol>
<h1 id="位置编码相关工作"><a href="#位置编码相关工作" class="headerlink" title="位置编码相关工作"></a>位置编码相关工作</h1><h2 id="总述"><a href="#总述" class="headerlink" title="总述"></a>总述</h2><p>基于自注意力机制本身的计算原理，其对位置信息不敏感，具体从公式角度来看：</p>
<p><img src="/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.assets/image.png" alt="Attention-cal"></p>
<p>调换序列中两个元素的位置不会影响到当前的注意力得分计算。</p>
<h3 id="绝对位置编码"><a href="#绝对位置编码" class="headerlink" title="绝对位置编码"></a>绝对位置编码</h3><ul>
<li>训练式</li>
</ul>
<p>直接将位置编码当作可训练参数。一般的认为，该方法的缺点在于没有外推性，超过预设窗口大小的内容就无法处理了。(当前有一些可以通过如层次分解的方法将位置编码外推足够长的范围)</p>
<ul>
<li>三角式-transformer：</li>
</ul>
<p><img src="/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.assets/image-20230718230559320.png" alt="image-20230718230559320"></p>
<p><img src="/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.assets/image-20230718230640108.png" alt="image-20230718230640108"></p>
<p>通过内积的方法将相对位置信息融入到特征中。但具体实现中，由于参数矩阵也需要参与计算$p^{T}<em>{t}W^{T}</em>{Q}W_{K}p_s \not &#x3D;{p^{T}_{t}p_s}.$经相关研究，可知由于参数矩阵使得余弦波不再是理想情况，无法真正感知元素的相对未知信息。</p>
<ul>
<li>递归式</li>
</ul>
<p>本质思想是RNN这类递归模型，学习位置编码，再接入transformer。但最大的问题是牺牲了一定的并行性，会带来速度瓶颈。</p>
<h3 id="相对位置编码"><a href="#相对位置编码" class="headerlink" title="相对位置编码"></a>相对位置编码</h3><p>相对位置编码在计算自注意力矩阵时，根据矩阵元素的下标，直接考虑每个元素对应的两个token间的相对位置关系。此外，相比于绝对位置编码仅仅在输入层考虑顺序特征，相对位置编码则通过修改自注意力计算的过程，植入到Transformer架构的每一层。</p>
<p>不同模型的相对位置编码：</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/631363482">Transformer位置编码（基础）</a></li>
<li><a target="_blank" rel="noopener" href="https://kexue.fm/archives/8130">让研究人员绞尽脑汁的Transformer位置编码</a></li>
<li></li>
</ol>
<h2 id="RoPE"><a href="#RoPE" class="headerlink" title="RoPE"></a><a target="_blank" rel="noopener" href="https://gitee.com/Jonny-Jaia/ready-blog/blob/master/%E6%9D%82/4-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E4%B9%A0/LongContextWindow.md#rope">RoPE</a></h2><h1 id="不同激活函数与优化器设计"><a href="#不同激活函数与优化器设计" class="headerlink" title="不同激活函数与优化器设计"></a>不同激活函数与优化器设计</h1><h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p>AdamW</p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="SwiGLU"><a href="#SwiGLU" class="headerlink" title="SwiGLU"></a>SwiGLU</h3><ol>
<li>siwish-线性函数与ReLU之间的平滑</li>
</ol>
<p>$f(x)&#x3D;x \times sigmoid(\beta x)$</p>
<ol start="2">
<li>GELU-高斯误差线性单元，RELU的变种</li>
</ol>
<p>$f(x)&#x3D;x \times \phi(x),\phi(x)是正态分布的累积函数$，和Swish形式性质相似，表现相当</p>
<ol start="3">
<li>GLU-门控<br>$\text{GLU}(a, b) &#x3D; a \otimes \sigma(b)$<br><img src="/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.assets/v2-1520514ff5f2c6ea2a0def6ace10bb42_720w.png"></li>
</ol>
<p>具体就是首先通过中间向量g(x)&#x3D;xW进行门控操作，使用Sigmoid函数σ将其映射到0到1之间的范围，表示每个元素被保留的概率。然后，将输入向量x与门控后的向量进行逐元素相乘（即 ⊗ 操作），得到最终的输出向量。</p>
<ol start="4">
<li>GEGL-GLU变体</li>
</ol>
<p>就是将GLU中的sigmoid激活函数替换成GELU激活函数<br>5. SwiGLU<br><img src="/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.assets/v2-a7a7b4ad7ce1018183e1b8eb654f3f91_1440w.webp" alt="swishglu"></p>
<p>就是将GLU中的sigmoid激活函数替换成Swish激活函数</p>
<h3 id="参考-1"><a href="#参考-1" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/638735903">大语言模型综述&lt;演进，技术路线，区别，微调，实践，潜在问题与讨论&gt;</a></li>
</ol>
<h1 id="模型结构微调"><a href="#模型结构微调" class="headerlink" title="模型结构微调"></a>模型结构微调</h1><h2 id="Norm"><a href="#Norm" class="headerlink" title="Norm"></a>Norm</h2><h3 id="Pre-Norm"><a href="#Pre-Norm" class="headerlink" title="Pre-Norm"></a>Pre-Norm</h3><ul>
<li>LayerNorm会影响训练的稳定性</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://readpaper.com/pdf-annotate/note?pdfId=4500336028413485057&noteId=1867628851738139648">Megatron-LM</a> 用实验证明layernorm后置效果要更加稳定</p>
</blockquote>
<p><img src="/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.assets/image-20230712101727279.png" alt="image-20230712101727279"></p>
<p>Post-LN（原始的BERT）</p>
<p>Pre-LN：On layer normalization in the transformer architecture</p>
<p>Sandwich-LN: Cogview: Mastering text-to-image generation via transformers</p>
<p>通常认为稳定性上: Sandwich-LN &gt; Pre-LN &gt; Post-LN</p>
<h3 id="Norm类型"><a href="#Norm类型" class="headerlink" title="Norm类型"></a>Norm类型</h3><ul>
<li><p>LayerNorm<br>传统transformer-Post-LN、随着层数加深梯度范数会增大导致训练不稳定。<br>Pre-LN:使用pre LN的深层transformer训练更稳定，可以缓解训练不稳定问题。但缺点是pre LN可能会轻微影响transformer模型的性能 大语言模型的一个挑战就是如何提升训练的稳定性。<br><img src="/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.assets/v2-dcd81bd8ea2fcba1e777f700ac1e5146_1440w.webp" alt="LayerNorm"></p>
<p>Norm中采用的性质：</p>
<ol>
<li>平移不变性：均值</li>
<li>缩放不变性：方差</li>
</ol>
</li>
<li><p>RMSNorm<br>只保留缩放，简化计算的同时，效果基本相当甚至还略有提升。<br><img src="/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.assets/v2-f1eb53988e49ac7358d2af489ec4b9bd_1440w.webp" alt="RMSNorm"></p>
</li>
<li><p>DeepNorm</p>
<p><img src="/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.assets/98a70456797347a68085cce705d71ed3.png" alt="deepnorm"></p>
<p>用以缓解爆炸式模型更新的问题，更可以再此基础上实现千层堆积。<br>$x &#x3D; LayerNorm(x \times \alpha + f(x))$</p>
<ol>
<li>DeepNorm在进行Layer Norm之前会以 α参数扩大残差连接</li>
<li>在Xavier参数初始化过程中以 β减小部分参数的初始化范围</li>
</ol>
</li>
</ul>
<h3 id="参考-2"><a href="#参考-2" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/638735903">大语言模型综述&lt;演进，技术路线，区别，微调，实践，潜在问题与讨论&gt;</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/479860623">DEEPNORM：千层transformer…</a></li>
</ol>
<h1 id="不同attention实现"><a href="#不同attention实现" class="headerlink" title="不同attention实现"></a>不同attention实现</h1><h2 id="Flash-Attention"><a href="#Flash-Attention" class="headerlink" title="Flash-Attention"></a><a target="_blank" rel="noopener" href="https://gitee.com/Jonny-Jaia/ready-blog/blob/master/%E6%9D%82/4-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E4%B9%A0/FlashAttention.md#flashattention">Flash-Attention</a></h2><h2 id="PageAttention"><a href="#PageAttention" class="headerlink" title="PageAttention"></a><a target="_blank" rel="noopener" href="https://gitee.com/Jonny-Jaia/ready-blog/blob/master/%E6%9D%82/4-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E4%B9%A0/LLM%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F.md#vllmpagedattention">PageAttention</a></h2><h2 id="Multi-Query-Attention"><a href="#Multi-Query-Attention" class="headerlink" title="Multi-Query Attention"></a><a href="">Multi-Query Attention</a></h2><h2 id="Group-Query-Attention"><a href="#Group-Query-Attention" class="headerlink" title="Group-Query Attention"></a><a href="">Group-Query Attention</a></h2><h1 id="Bert与GPT的不同"><a href="#Bert与GPT的不同" class="headerlink" title="Bert与GPT的不同"></a>Bert与GPT的不同</h1><h2 id="自回归模型"><a href="#自回归模型" class="headerlink" title="自回归模型"></a>自回归模型</h2><p>AR，代表GPT，从左向右学习。</p>
<p>AR模型通常用于生成式任务，在长文本的生成能力很强，比如自然语言生成（NLG）领域的任务：摘要、翻译或抽象问答。</p>
<p>具体来说，就是利用上文词预测下一个词的发生概率。</p>
<p>优点：AR模型擅长生成式NLP任务。AR模型使用注意力机制，预测下一个token，因此自然适用于文本生成。此外，AR模型可以简单地将训练目标设置为预测语料库中的下一个token，因此生成数据相对容易。</p>
<p>缺点：AR模型只能用于前向或者后向建模，不能同时使用双向的上下文信息，不能完全捕捉token的内在联系。</p>
<h3 id="GPT系列模型"><a href="#GPT系列模型" class="headerlink" title="GPT系列模型"></a>GPT系列模型</h3><ul>
<li><p>GPT1<br>通过无监督预训练+有监督微调实现模型性能的提升。另外，将预训练目标作为辅助目标加入下游任务loss中，将会提高有监督模型的泛化性能，并加速收敛。</p>
</li>
<li><p>GPT2-15B</p>
</li>
</ul>
<blockquote>
<p> “所有的有监督学习都是无监督语言模型的一个子集”</p>
</blockquote>
<p>  增大了模型大小与参数规模，提出了zero-shot，并且提出了以一个通用预训练模型为基础，使得下游任务无需手动生成或标记训练数据集，更不需要修改预训练模型的参数或结构。</p>
<p>  GPT2通过实验验证了海量数据与大量参数训练得到的语言模型可以迁移到下游其他任务中，无需额外训练和微调。</p>
<ul>
<li><p>GPT3-175B<br>引入了In-Context Learning的概念，GPT3参数量增大的同时，期望不通过微调直接能够通过上下文指示也能够有较好的性能。</p>
<p>In-Context learning是元学习（Meta-learning）的一种，元学习的核心思想在于通过<strong>少量的数据寻找一个合适的初始化范围</strong>，使得模型能够在有限的数据集上快速拟合，并获得不错的效果。</p>
</li>
<li><p>InstructGPT</p>
<blockquote>
<p>提出动机：让模型的输出达到3H(helpful,honest,harmless)</p>
</blockquote>
<ol>
<li>RLHF</li>
</ol>
<p>  人类喜欢的内容大致符合以上的3H标准，并且也能够保证生成内容流畅性与语法正确性；</p>
<p>  通过RL指导模型训练，以人类反馈作为奖励，实现将人类经验内容的注入。</p>
<ol start="2">
<li>实验步骤</li>
</ol>
<p>  有监督微调-基于人工标注的对比数据训练奖励模型-基于RM利用PPO微调SFT模型；</p>
<p>  三部分数据集：<br>  SFT数据：简单任务、few-shot任务、用户相关的任务；<br>  RM数据：让模型先生成一批候选文本，然后针对这部分数据进行排序；<br>  PPO数据：无标注数据，来自GPT3的API用户调用任务数据；</p>
<p>  训练设置：<br>  SFT：与GPT3一致，适当过拟合有助于后续的训练；<br>  RM：输入prompt和response，输出奖励值；训练过程中将同一个prompt的k个输出成对取出共有$C_{K}^{2}$个结果作为一个batch输入，loss就是最大化结果的差值；<br>  PPO：KL惩罚确保两个策略的输出差距不会很大；为了防止模型在通用NLP任务上性能大幅度下降，优化目标中增加了通用语言模型的目标；</p>
<ol start="3">
<li>优缺点</li>
</ol>
<p>  优点：结果更真实，无害性提高，coding能力提升；<br>  缺点：会降低在通用NLP任务上的效果；依然会给出奇怪的输出；对指示十分敏感；对简单概念过分解读。</p>
</li>
<li><p>GPT4-1.8T</p>
</li>
</ul>
<ol>
<li>模型架构</li>
</ol>
<p>采用的是MoE的架构</p>
<ol start="2">
<li>数据组成</li>
</ol>
<h2 id="自编码模型"><a href="#自编码模型" class="headerlink" title="自编码模型"></a>自编码模型</h2><p>AE，代表BERT，主要是对掩码部分能够实现重建，常用于内容理解任务，比如自然语言理解（NLU）中的分类任务：情感分析、提取式问答。</p>
<p>优点：在上下文依赖中，BERT的表示可以涵盖前后向两边的上下文。BERT使用双向transformer，在语言理解相关的任务中表现很好。</p>
<p>缺点：</p>
<ul>
<li>输入噪声：<br>BERT在预训练过程中使用【mask】符号对输入进行处理，这些符号在下游的finetune任务中永远不会出现，这会导致预训练-微调差异。而AR模型不会依赖于任何被mask的输入，因此不会遇到这类问题。</li>
<li>BERT在对联合条件概率进行因式分解时，基于一个独立假设：在给定了unmasked tokens时，所有待预测（masked）的tokens是相互独立的。</li>
</ul>
<h2 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h2><p>同时包含编码器和解码器两部分，常用的有T5、BART等模型</p>
<h2 id="参考-3"><a href="#参考-3" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/626494749">[万字长文]ChatGPT系列论文精读——大模型经典论文GPT1、GPT2、GPT3</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/625714067">一文读懂GPT家族和BERT的底层区别——自回归和自编码语言模型详解</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/642927542">大规模语言模型（LLMs）预训练十六: GPT-4大揭密</a></li>
<li><a target="_blank" rel="noopener" href="https://www.semianalysis.com/p/gpt-4-architecture-infrastructure">GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE (semianalysis.com)</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/590311003">ChatGPT&#x2F;InstructGPT详解</a></li>
</ol>

  </article>
  <!-- tag -->
  <div class="mt-12 pt-6 border-t border-gray-200">
    
      
        <span class="bg-gray-100 dark:bg-gray-700 px-2 py-1 m-1 text-sm rounded-md transition-colors hover:bg-gray-200">
          <a href="/tags/Transformer%E6%A8%A1%E5%9E%8B-self-attention/">Transformer模型 - self-attention</a>
        </span>
      
    
  </div>
  <!-- prev and next -->
  <div class="flex justify-between mt-12 pt-6 border-t border-gray-200">
    <div>
      
        <a href="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/" class="text-sm text-gray-400 hover:text-gray-500 flex justify-center">
          <iconify-icon width="20" icon="ri:arrow-left-s-line" data-inline="false"></iconify-icon>
          生成式模型学习
        </a>
      
    </div>
    <div>
      
        <a href="/2023/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E6%96%B9%E5%90%91/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/" class="text-sm text-gray-400 hover:text-gray-500 flex justify-center">
          
          <iconify-icon width="20" icon="ri:arrow-right-s-line" data-inline="false"></iconify-icon>
        </a>
      
    </div>
  </div>
  <!-- comment -->
  <div class="article-comments mt-12">
    

  </div>
</section>
<!-- js inspect -->

<script src="/lib/clipboard.min.js"></script>


<script async src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
  });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>



<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>
  $(document).ready(() => {
    const maraidConfig = {
      theme: "default",
      logLevel: 3,
      flowchart: { curve: "linear" },
      gantt: { axisFormat: "%m/%d/%Y" },
      sequence: { actorMargin: 50 },
    };
    mermaid.initialize(maraidConfig);
  });
</script>



<script src="/lib/fancybox/fancybox.umd.min.js"></script>

<script>
  $(document).ready(() => {
    $('.post-content').each(function(i){
      $(this).find('img').each(function(){
        if ($(this).parent().hasClass('fancybox') || $(this).parent().is('a')) return;
        var alt = this.alt;
        if (alt) $(this).after('<span class="fancybox-alt">' + alt + '</span>');
        $(this).wrap('<a class="fancybox-img" href="' + this.src + '" data-fancybox=\"gallery\" data-caption="' + alt + '"></a>')
      });
      $(this).find('.fancybox').each(function(){
        $(this).attr('rel', 'article' + i);
      });
    });

    Fancybox.bind('[data-fancybox="gallery"]', {
        // options
    })
  })
</script>

<!-- tocbot begin -->

<script src="/lib/tocbot/tocbot.min.js"></script>

<script>
  $(document).ready(() => {
      tocbot.init({
        // Where to render the table of contents.
        tocSelector: '.post-toc',
        // Where to grab the headings to build the table of contents.
        contentSelector: '.post-content',
        // Which headings to grab inside of the contentSelector element.
        headingSelector: 'h1, h2, h3',
        // For headings inside relative or absolute positioned containers within content.
        hasInnerContainers: true,
    });
  })
</script>
<!-- tocbot end -->


  </main>
  <footer class="flex flex-col h-40 items-center justify-center text-gray-400 text-sm">
  <!-- busuanzi -->
  
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<!-- Busuanzi Analytics -->
<div class="flex items-center gap-2">
  <span>Visitors</span>
  <span id="busuanzi_value_site_uv"></span>
  <span>Page Views</span>
  <span id="busuanzi_value_site_pv"></span>
</div>
<!-- End Busuanzi Analytics -->


  <!-- copyright -->
  <div class="flex items-center gap-2">
    <a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" style="color: inherit;">CC BY-NC-SA 4.0</a>
    <span>© 2022</span>
    <iconify-icon width="18" icon="emojione-monotone:maple-leaf" ></iconify-icon>
    <a href="https://github.com/xbmlz" target="_blank" rel="noopener noreferrer">xbmlz</a>
  </div>
  <!-- powered by -->
  <div class="flex items-center gap-2">
    <span>Powered by</span>
    <a href="https://hexo.io/" target="_blank" rel="noopener noreferrer">Hexo</a>
    <span>&</span>
    <a href="https://github.com/xbmlz/hexo-theme-maple" target="_blank" rel="noopener noreferrer">Maple</a>
  </div>

</footer>

  <div class="back-to-top box-border fixed right-6 z-1024 -bottom-20 rounded py-1 px-1 bg-slate-900 opacity-60 text-white cursor-pointer text-center dark:bg-slate-600">
    <span class="flex justify-center items-center text-sm">
      <iconify-icon width="18" icon="ion:arrow-up-c" id="go-top"></iconify-icon>
      <span id="scrollpercent"><span>0</span> %</span>
    </span>
  </div>
  
<script src="/js/main.js"></script>


  <script>
    $(document).ready(function () {
      const mapleCount = "10";
      const speed = "0.5";
      const mapleEl = document.getElementById("maple");
      const maples = Array.from({ length: mapleCount }).map(() => {
        const maple = document.createElement("div");
        const scale = Math.random() * 0.5 + 0.5;
        const offset = Math.random() * 2 - 1;
        const x = Math.random() * mapleEl.clientWidth;
        const y = -Math.random() * mapleEl.clientHeight;
        const duration = 10 / speed;
        const delay = -duration;
        maple.className = "maple";
        maple.style.width = `${24 * scale}px`;
        maple.style.height = `${24 * scale}px`;
        maple.style.left = `${x}px`;
        maple.style.top = `${y}px`;
        maple.style.setProperty("--maple-fall-offset", offset);
        maple.style.setProperty("--maple-fall-height", `${Math.abs(y) + mapleEl.clientHeight}px`);
        maple.style.animation = `fall ${duration}s linear infinite`;
        maple.style.animationDelay = `${delay}s`;
        mapleEl.appendChild(maple)
        return maple
      })
    });
  </script>
  


  <div class="fixed top-0 bottom-0 left-0 right-0 pointer-events-none print:hidden" id="maple"></div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>
